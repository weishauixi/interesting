{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f3dcbc038f2bf61",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 深度学习基础教程\n",
    "\n",
    "本notebook介绍了PyTorch深度学习的核心概念和代码示例，包括：\n",
    "- 张量操作\n",
    "- 自动微分\n",
    "- 神经网络搭建\n",
    "- 损失函数与优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91aeb508e1428b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 张量的创建与基本操作\n",
    "\n",
    "import torch\n",
    "\n",
    "def demo_tensor_creation():\n",
    "    \"\"\"演示张量的基本创建方法\"\"\"\n",
    "    # 创建标量张量\n",
    "    t1 = torch.tensor(10)\n",
    "    print(\"标量张量:\")\n",
    "    print(t1)\n",
    "\n",
    "    # 创建列表张量\n",
    "    data = [[4, 5, 6], [1, 2, 3]]\n",
    "    t2 = torch.tensor(data)\n",
    "    print(\"\\n二维张量:\")\n",
    "    print(t2)\n",
    "    print(f\"类型: {type(t2)}\")\n",
    "\n",
    "def demo_tensor_special():\n",
    "    \"\"\"演示特殊张量的创建\"\"\"\n",
    "    # 创建未初始化的张量\n",
    "    t6 = torch.Tensor(3, 4)\n",
    "    print(\"\\n未初始化张量 (3x4):\")\n",
    "    print(t6)\n",
    "\n",
    "    # 创建指定类型的张量\n",
    "    t5 = torch.IntTensor(3, 4)\n",
    "    print(\"\\n整型张量 (3x4):\")\n",
    "    print(t5)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"=\" * 50)\n",
    "    print(\"【张量创建示例】\")\n",
    "    print(\"=\" * 50)\n",
    "    demo_tensor_creation()\n",
    "    demo_tensor_special()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2279874e8767b4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 创建线性和随机张量\n",
    "\n",
    "import torch\n",
    "\n",
    "def demo_sequence_tensors():\n",
    "    \"\"\"演示序列张量的创建\"\"\"\n",
    "    # arange: 创建等差数列\n",
    "    t1 = torch.arange(0, 10, 2)\n",
    "    print(\"arange创建的等差张量 [0, 10, 步长2]:\")\n",
    "    print(t1)\n",
    "    print(f\"类型: {type(t1)}\\n\")\n",
    "\n",
    "    # linspace: 创建线性等间距张量\n",
    "    t2 = torch.linspace(1, 10, 4)\n",
    "    print(\"linspace创建的等间距张量 [1, 10, 4个点]:\")\n",
    "    print(t2)\n",
    "\n",
    "def demo_random_tensors():\n",
    "    \"\"\"演示随机张量的创建\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"【随机张量示例】\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 均匀分布随机张量 [0, 1)\n",
    "    t1 = torch.rand(size=(2, 3))\n",
    "    print(\"\\n均匀分布随机张量 (2x3):\")\n",
    "    print(t1)\n",
    "\n",
    "    # 设置随机种子\n",
    "    torch.manual_seed(3)\n",
    "    t1_seeded = torch.rand(size=(2, 3))\n",
    "    print(\"\\n设置种子后的随机张量:\")\n",
    "    print(t1_seeded)\n",
    "\n",
    "    # 正态分布随机张量\n",
    "    t2 = torch.randn(size=(2, 3))\n",
    "    print(\"\\n正态分布随机张量 (2x3):\")\n",
    "    print(t2)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    demo_sequence_tensors()\n",
    "    demo_random_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d26c6814bbe0689d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始张量 (float32):\n",
      "tensor([1., 2., 3., 4., 5.])\n",
      "类型: torch.float32\n",
      "\n",
      "转换为int16:\n",
      "tensor([1, 2, 3, 4, 5], dtype=torch.int16)\n",
      "类型: torch.int16\n",
      "\n",
      "=== 类型转换示例 ===\n",
      "half() (float16): torch.float16\n",
      "double() (float64): torch.float64\n",
      "short() (int16): torch.int16\n",
      "long() (int64): torch.int64\n"
     ]
    }
   ],
   "source": [
    "# 3. 张量类型转换\n",
    "\n",
    "import torch\n",
    "\n",
    "# 创建浮点型张量\n",
    "t1 = torch.tensor([1, 2, 3, 4, 5], dtype=torch.float32)\n",
    "print(\"原始张量 (float32):\")\n",
    "print(t1)\n",
    "print(f\"类型: {t1.dtype}\\n\")\n",
    "\n",
    "# 转换为int16\n",
    "t2 = t1.type(torch.int16)\n",
    "print(\"转换为int16:\")\n",
    "print(t2)\n",
    "print(f\"类型: {t2.dtype}\\n\")\n",
    "\n",
    "# 演示各种类型转换方法\n",
    "print(\"=== 类型转换示例 ===\")\n",
    "print(f\"half() (float16): {t2.half().dtype}\")\n",
    "print(f\"double() (float64): {t2.double().dtype}\")\n",
    "print(f\"short() (int16): {t2.short().dtype}\")\n",
    "print(f\"long() (int64): {t2.long().dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ff2a84633625cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 创建全0、全1和填充张量\n",
    "\n",
    "import torch\n",
    "\n",
    "print(\"=== 全1张量 ===\")\n",
    "t1 = torch.ones(2, 3)\n",
    "print(f\"ones(2, 3):\\n{t1}\\n\")\n",
    "\n",
    "print(\"=== 全1张量（参照给定张量形状）===\")\n",
    "t2 = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
    "t3 = torch.ones_like(t2)\n",
    "print(f\"参照张量:\\n{t2}\")\n",
    "print(f\"\\nones_like结果:\\n{t3}\\n\")\n",
    "\n",
    "print(\"=== 填充张量 ===\")\n",
    "t4 = torch.full(size=(2, 3), fill_value=255)\n",
    "print(f\"full((2,3), 255):\\n{t4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9448d954f0c456a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 张量与NumPy数组转换\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def demo_tensor_to_numpy():\n",
    "    \"\"\"演示张量转NumPy\"\"\"\n",
    "    print(\"=== 张量 → NumPy ===\")\n",
    "    t1 = torch.tensor([1, 2, 3, 4, 5])\n",
    "    n1 = t1.numpy()\n",
    "    print(f\"原始张量: {t1}\")\n",
    "    print(f\"转换后NumPy: {n1}\")\n",
    "    \n",
    "    # 注意：共享内存\n",
    "    n1[0] = 100\n",
    "    print(f\"\\n修改NumPy后:\")\n",
    "    print(f\"NumPy: {n1}\")\n",
    "    print(f\"张量（已改变）: {t1}\")\n",
    "\n",
    "def demo_numpy_to_tensor():\n",
    "    \"\"\"演示NumPy转张量\"\"\"\n",
    "    print(\"\\n=== NumPy → 张量 ===\")\n",
    "    n1 = np.array([11, 22, 33])\n",
    "    print(f\"原始NumPy: {n1}\")\n",
    "    \n",
    "    # 方法1: from_numpy (共享内存)\n",
    "    t1 = torch.from_numpy(n1)\n",
    "    print(f\"from_numpy: {t1}\")\n",
    "    \n",
    "    # 方法2: torch.tensor (不共享内存)\n",
    "    t2 = torch.tensor(n1)\n",
    "    print(f\"torch.tensor: {t2}\")\n",
    "\n",
    "def demo_scalar_extraction():\n",
    "    \"\"\"演示标量提取\"\"\"\n",
    "    print(\"\\n=== 标量提取 ===\")\n",
    "    t1 = torch.tensor(100)\n",
    "    print(f\"张量: {t1}\")\n",
    "    \n",
    "    a = t1.item()\n",
    "    print(f\"使用.item()提取: {a} (类型: {type(a)})\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    demo_tensor_to_numpy()\n",
    "    demo_numpy_to_tensor()\n",
    "    demo_scalar_extraction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43513eb2a7240f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 张量的基本运算\n",
    "\n",
    "import torch\n",
    "\n",
    "t1 = torch.tensor([1, 2, 3, 4, 5])\n",
    "print(f\"原始张量: {t1}\\n\")\n",
    "\n",
    "print(\"=== 基本算术运算 ===\")\n",
    "print(f\"取反 (neg()): {t1.neg()}\")\n",
    "print(f\"加法 (+2): {t1 + 2}\")\n",
    "print(f\"减法 (-2): {t1 - 2}\")\n",
    "print(f\"乘法 (*2): {t1 * 2}\")\n",
    "print(f\"除法 (/2): {t1 / 2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3aea585885417bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 点乘与矩阵乘法\n",
    "\n",
    "import torch\n",
    "\n",
    "def demo_element_wise_multiply():\n",
    "    \"\"\"演示逐元素乘法（点乘）\"\"\"\n",
    "    print(\"=== 逐元素乘法 (Element-wise Multiply) ===\")\n",
    "    t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "    t2 = torch.tensor([[2, 3, 4], [1, 2, 3]])\n",
    "    \n",
    "    print(f\"张量1:\\n{t1}\")\n",
    "    print(f\"\\n张量2:\\n{t2}\")\n",
    "    print(f\"\\n点乘结果 (t1 * t2):\\n{t1 * t2}\")\n",
    "\n",
    "def demo_matrix_multiply():\n",
    "    \"\"\"演示矩阵乘法\"\"\"\n",
    "    print(\"\\n=== 矩阵乘法 (Matrix Multiplication) ===\")\n",
    "    t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])  # (2, 3)\n",
    "    t2 = torch.tensor([[1, 2], [3, 4], [5, 6]])  # (3, 2)\n",
    "    \n",
    "    print(f\"张量1 (2x3):\\n{t1}\")\n",
    "    print(f\"\\n张量2 (3x2):\\n{t2}\")\n",
    "    print(f\"\\n矩阵乘法结果 (2x2):\\n{t1 @ t2}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    demo_element_wise_multiply()\n",
    "    demo_matrix_multiply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03cbd741c1cd165",
   "metadata": {},
   "source": [
    "## 8. 张量索引\n",
    "\n",
    "张量索引是获取张量中特定元素的重要操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476b2146c36dfc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 张量索引示例\n",
    "\n",
    "import torch\n",
    "\n",
    "# 设置随机种子，确保结果可复现\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 创建5x5的随机张量\n",
    "t1 = torch.randint(1, 10, (5, 5))\n",
    "print(\"原始张量 (5x5):\")\n",
    "print(t1)\n",
    "print()\n",
    "\n",
    "print(\"=== 基础索引 ===\")\n",
    "print(f\"t1[1] (第2行): {t1[1]}\")\n",
    "print(f\"t1[:1, 2] (第1行，第3列): {t1[:1, 2]}\")\n",
    "print()\n",
    "\n",
    "print(\"=== 高级索引 ===\")\n",
    "print(f\"t1[[1,3], [2,4]] (第2行第3列 + 第4行第5列): {t1[[1,3], [2,4]]}\")\n",
    "print()\n",
    "\n",
    "print(\"=== 条件索引 ===\")\n",
    "# 第3列大于5的行数据\n",
    "col_3 = t1[:, 2]\n",
    "mask = col_3 > 5\n",
    "print(f\"第3列: {col_3}\")\n",
    "print(f\"第3列大于5的行索引: {mask.nonzero().squeeze()}\")\n",
    "print(f\"满足条件的数据: {col_3[mask]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37bd2dd5705828b",
   "metadata": {},
   "source": [
    "## 9. 张量的形状操作\n",
    "\n",
    "形状操作是深度学习中非常重要的操作，包括reshape、unsqueeze、squeeze、transpose等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb04b6ac921c4eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 张量的形状操作\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def demo_reshape():\n",
    "    \"\"\"演示reshape操作\"\"\"\n",
    "    print(\"=== Reshape（改变形状） ===\")\n",
    "    t1 = torch.randint(1, 10, (2, 3))\n",
    "    print(f\"原始张量 (2x3):\\n{t1}\")\n",
    "    print(f\"形状: {t1.shape}\\n\")\n",
    "    \n",
    "    t2 = t1.reshape(3, 2)\n",
    "    print(f\"reshape为(3, 2):\\n{t2}\")\n",
    "    print(f\"形状: {t2.shape}\")\n",
    "\n",
    "def demo_unsqueeze():\n",
    "    \"\"\"演示unsqueeze操作（增加维度）\"\"\"\n",
    "    print(\"\\n=== Unsqueeze（增加维度） ===\")\n",
    "    t1 = torch.randint(1, 10, (2, 3))\n",
    "    print(f\"原始张量形状: {t1.shape}\\n\")\n",
    "    \n",
    "    # 在dim=1位置增加维度\n",
    "    t2 = t1.unsqueeze(1)\n",
    "    print(f\"unsqueeze(1)后形状: {t2.shape}\")  # (2, 1, 3)\n",
    "    \n",
    "    # 在dim=2位置增加维度\n",
    "    t3 = t1.unsqueeze(2)\n",
    "    print(f\"unsqueeze(2)后形状: {t3.shape}\")  # (2, 3, 1)\n",
    "\n",
    "def demo_transpose():\n",
    "    \"\"\"演示transpose操作（交换维度）\"\"\"\n",
    "    print(\"\\n=== Transpose（交换维度） ===\")\n",
    "    t1 = torch.randint(1, 10, (2, 3, 4))\n",
    "    print(f\"原始张量形状: {t1.shape}\\n\")\n",
    "    \n",
    "    # 交换第0维和最后一维\n",
    "    t2 = t1.transpose(0, -1)\n",
    "    print(f\"transpose(0, -1)后形状: {t2.shape}\")\n",
    "    \n",
    "    # 说明：原来(2,3,4) → 交换后(4,3,2)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    demo_reshape()\n",
    "    demo_unsqueeze()\n",
    "    demo_transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1842294d16d17b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. 自动微分基础\n",
    "\n",
    "import torch\n",
    "\n",
    "# 创建需要计算梯度的张量\n",
    "w = torch.tensor(10.0, requires_grad=True, dtype=torch.float32)\n",
    "\n",
    "# 定义损失函数: loss = 2 * w²\n",
    "loss = 2 * w ** 2\n",
    "\n",
    "print(f\"权重 w: {w}\")\n",
    "print(f\"损失函数: loss = 2 * w²\")\n",
    "print(f\"loss.grad_fn: {loss.grad_fn}\")\n",
    "print(f\"梯度函数类型: {type(loss.grad_fn)}\\n\")\n",
    "\n",
    "# 反向传播，计算梯度\n",
    "loss.backward()\n",
    "\n",
    "print(f\"∂loss/∂w = {w.grad}\")\n",
    "\n",
    "# 梯度下降更新\n",
    "w_new = w.data - 0.01 * w.grad\n",
    "print(f\"\\n更新后的权重: {w_new}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3236e16296a3008b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. 自动微分案例 - 梯度下降优化\n",
    "\n",
    "import torch\n",
    "\n",
    "# 初始化权重\n",
    "w = torch.tensor(10.0, requires_grad=True, dtype=torch.float32)\n",
    "\n",
    "print(\"=== 梯度下降优化过程 ===\")\n",
    "print(f\"初始权重: {w}\")\n",
    "print(f\"目标: 最小化 loss = w² + 20\\n\")\n",
    "\n",
    "# 梯度下降循环100次\n",
    "print(\"迭代 | 损失值 | 梯度 | 权重\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for i in range(1, 101):\n",
    "    # 正向传播：计算损失\n",
    "    loss = w ** 2 + 20\n",
    "    \n",
    "    # 梯度清零（重要！）\n",
    "    if w.grad is not None:\n",
    "        w.grad.zero_()\n",
    "    \n",
    "    # 反向传播：计算梯度\n",
    "    loss.backward()\n",
    "    \n",
    "    # 权重更新: w = w - learning_rate * gradient\n",
    "    w.data = w.data - 0.01 * w.grad\n",
    "    \n",
    "    # 每10次迭代打印一次\n",
    "    if i % 10 == 0 or i == 1:\n",
    "        print(f\"{i:4d} | {loss.item():8.4f} | {w.grad.item():6.4f} | {w.data.item():6.4f}\")\n",
    "\n",
    "print(f\"\\n最终结果:\")\n",
    "print(f\"最优权重: {w.data.item():.4f} (理论最优值: 0.0000)\")\n",
    "print(f\"最终损失: {(w.data ** 2 + 20).item():.4f} (理论最小值: 20.0000)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48e1fe6e5d09cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. detach()函数使用\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(\"=== detach()函数说明 ===\")\n",
    "print(\"detach()用于从计算图中分离张量，返回一个不需要梯度的副本\\n\")\n",
    "\n",
    "# 创建需要梯度的张量\n",
    "t1 = torch.tensor([10.0, 20.0], requires_grad=True, dtype=torch.float32)\n",
    "print(f\"原始张量: {t1}\")\n",
    "print(f\"requires_grad: {t1.requires_grad}\\n\")\n",
    "\n",
    "# 使用detach分离\n",
    "t2 = t1.detach()\n",
    "print(f\"detach后的张量: {t2}\")\n",
    "print(f\"requires_grad: {t2.requires_grad}\")\n",
    "print(f\"形状: {t2.shape}\\n\")\n",
    "\n",
    "# 修改原张量\n",
    "t1.data[0] = 100\n",
    "print(f\"修改原张量后:\")\n",
    "print(f\"原张量: {t1}\")\n",
    "print(f\"detach张量（共享内存）: {t2}\\n\")\n",
    "\n",
    "# detach后可以直接转为numpy\n",
    "n1 = t2.numpy()\n",
    "print(f\"转为NumPy数组: {n1}\")\n",
    "print(f\"类型: {type(n1)}\\n\")\n",
    "\n",
    "# 一行代码完成\n",
    "print(\"=== 一行代码示例 ===\")\n",
    "t3 = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "n2 = t3.detach().numpy()\n",
    "print(f\"tensor → detach → numpy: {n2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413256c1832bbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. 完整的前向传播与反向传播流程\n",
    "\n",
    "import torch\n",
    "\n",
    "print(\"=== 神经网络训练基本流程 ===\\n\")\n",
    "\n",
    "# 1. 准备数据\n",
    "x = torch.ones(2, 5)  # 2个样本，5个特征\n",
    "y = torch.zeros(2, 3)  # 真实标签\n",
    "\n",
    "print(\"输入 x (2x5):\")\n",
    "print(x)\n",
    "print(\"\\n真实标签 y (2x3):\")\n",
    "print(y)\n",
    "\n",
    "# 2. 初始化参数（需要梯度）\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "\n",
    "print(f\"\\n权重 w (5x3):\")\n",
    "print(w.data)\n",
    "print(f\"\\n偏置 b (3):\")\n",
    "print(b.data)\n",
    "\n",
    "# 3. 前向传播\n",
    "z = torch.matmul(x, w) + b\n",
    "print(f\"\\n预测值 z (2x3):\")\n",
    "print(z.data)\n",
    "\n",
    "# 4. 计算损失\n",
    "criterion = torch.nn.MSELoss()\n",
    "loss = criterion(z, y)\n",
    "print(f\"\\n损失值: {loss.item():.4f}\")\n",
    "\n",
    "# 5. 反向传播\n",
    "loss.backward()\n",
    "\n",
    "print(f\"\\n=== 反向传播后 ===\")\n",
    "print(f\"权重梯度:\\n{w.grad}\")\n",
    "print(f\"\\n偏置梯度: {b.grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7138e794570c4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. 线性回归完整案例\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch import nn, optim\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# 设置中文字体和图形样式\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['figure.figsize'] = (12, 5)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "def create_dataset():\n",
    "    \"\"\"创建回归数据集\"\"\"\n",
    "    print(\"=== 创建数据集 ===\")\n",
    "    x, y, coef = make_regression(\n",
    "        n_samples=100,\n",
    "        n_features=1,\n",
    "        noise=10,\n",
    "        coef=True,\n",
    "        random_state=42\n",
    "    )\n",
    "    x_tensor = torch.tensor(x, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
    "    \n",
    "    print(f\"样本数: {len(x)}\")\n",
    "    print(f\"真实系数: {coef:.4f}\")\n",
    "    print(f\"x范围: [{x.min():.2f}, {x.max():.2f}]\")\n",
    "    print(f\"y范围: [{y.min():.2f}, {y.max():.2f}]\\n\")\n",
    "    \n",
    "    return x_tensor, y_tensor, coef\n",
    "\n",
    "def train_model(x, y, true_coef):\n",
    "    \"\"\"训练线性回归模型\"\"\"\n",
    "    print(\"=== 训练模型 ===\")\n",
    "    \n",
    "    # 创建数据集和数据加载器\n",
    "    dataset = TensorDataset(x, y)\n",
    "    dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "    \n",
    "    # 创建模型\n",
    "    model = nn.Linear(1, 1)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    \n",
    "    # 训练\n",
    "    epochs = 100\n",
    "    loss_history = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for batch_x, batch_y in dataloader:\n",
    "            # 前向传播\n",
    "            y_pred = model(batch_x)\n",
    "            loss = criterion(y_pred, batch_y)\n",
    "            \n",
    "            # 反向传播\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        loss_history.append(avg_loss)\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # 打印训练结果\n",
    "    trained_weight = model.weight.data.item()\n",
    "    trained_bias = model.bias.data.item()\n",
    "    print(f\"\\n训练结果:\")\n",
    "    print(f\"学习到的权重: {trained_weight:.4f} (真实值: {true_coef:.4f})\")\n",
    "    print(f\"学习到的偏置: {trained_bias:.4f}\")\n",
    "    \n",
    "    return model, loss_history\n",
    "\n",
    "def visualize_results(x, y, model, true_coef, loss_history):\n",
    "    \"\"\"可视化训练结果\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # 子图1: 损失曲线\n",
    "    axes[0].plot(loss_history, linewidth=2, color='#2E86AB')\n",
    "    axes[0].set_title('训练损失曲线', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_xlabel('Epoch', fontsize=10)\n",
    "    axes[0].set_ylabel('Loss (MSE)', fontsize=10)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].set_facecolor('#F8F9FA')\n",
    "    \n",
    "    # 子图2: 回归拟合\n",
    "    axes[1].scatter(x.numpy(), y.numpy(), alpha=0.6, s=50, \n",
    "                   color='#A23B72', label='真实数据', edgecolors='white', linewidth=0.5)\n",
    "    \n",
    "    # 预测线\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(x).numpy()\n",
    "    axes[1].plot(x.numpy(), y_pred, linewidth=3, color='#2E86AB', \n",
    "                label=f'预测模型 (y={model.weight.item():.2f}x+{model.bias.item():.2f})')\n",
    "    \n",
    "    # 真实线\n",
    "    y_true = true_coef * x.numpy() + (y.mean() - true_coef * x.mean())\n",
    "    axes[1].plot(x.numpy(), y_true, linewidth=2, color='#18A558', \n",
    "                linestyle='--', alpha=0.7, label=f'真实模型 (y={true_coef:.2f}x+...)')\n",
    "    \n",
    "    axes[1].set_title('线性回归拟合结果', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_xlabel('X', fontsize=10)\n",
    "    axes[1].set_ylabel('Y', fontsize=10)\n",
    "    axes[1].legend(fontsize=9, loc='best')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].set_facecolor('#F8F9FA')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 创建数据\n",
    "    x, y, coef = create_dataset()\n",
    "    \n",
    "    # 训练模型\n",
    "    model, loss_history = train_model(x, y, coef)\n",
    "    \n",
    "    # 可视化结果\n",
    "    visualize_results(x, y, model, coef, loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "936180f1ec9c4783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 均匀分布初始化 ===\n",
      "权重范围: [0.0865, 0.9155]\n",
      "权重:\n",
      "tensor([[0.1997, 0.1322, 0.9155, 0.8094, 0.0865],\n",
      "        [0.1470, 0.8928, 0.2254, 0.6052, 0.7197],\n",
      "        [0.4144, 0.7101, 0.3649, 0.6261, 0.1172]])\n",
      "\n",
      "=== 全0初始化 ===\n",
      "权重均值: 0.0000\n",
      "权重:\n",
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "\n",
      "=== 全1初始化 ===\n",
      "权重均值: 1.0000\n",
      "权重:\n",
      "tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "\n",
      "=== Kaiming初始化（适合ReLU） ===\n",
      "权重标准差: 0.7981\n",
      "权重:\n",
      "tensor([[ 0.9854, -0.3329, -0.6604, -0.6699, -0.7940],\n",
      "        [ 1.0712, -0.1923,  0.7324, -0.9677, -0.2668],\n",
      "        [ 0.8341, -0.2812, -1.0502, -1.0444,  0.9068]])\n",
      "\n",
      "=== Xavier初始化（适合Sigmoid/Tanh） ===\n",
      "权重标准差: 0.4525\n",
      "权重:\n",
      "tensor([[-0.2798,  0.7641,  0.7635, -0.1113, -0.0521],\n",
      "        [ 0.0877, -0.0993,  0.6817,  0.3607,  0.5890],\n",
      "        [-0.7442,  0.3420, -0.1676,  0.7088,  0.3608]])\n"
     ]
    }
   ],
   "source": [
    "# 15. 神经网络参数初始化\n",
    "\n",
    "# 参数初始化是训练神经网络的关键步骤，目的包括：\n",
    "# 1. **防止梯度消失和爆炸**\n",
    "# 2. **提高收敛速度**\n",
    "# 3. **打破对称性**\n",
    "\n",
    "# **选择建议：**\n",
    "# - 使用ReLU激活函数 → 优先用Kaiming初始化\n",
    "# - 使用Sigmoid/Tanh激活函数 → 优先用Xavier初始化\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def demo_uniform_init():\n",
    "    \"\"\"演示均匀分布初始化\"\"\"\n",
    "    print(\"=== 均匀分布初始化 ===\")\n",
    "    linear = nn.Linear(5, 3)\n",
    "    nn.init.uniform_(linear.weight, a=0, b=1)\n",
    "    nn.init.uniform_(linear.bias)\n",
    "    print(f\"权重范围: [{linear.weight.data.min():.4f}, {linear.weight.data.max():.4f}]\")\n",
    "    print(f\"权重:\\n{linear.weight.data}\\n\")\n",
    "\n",
    "def demo_zeros_init():\n",
    "    \"\"\"演示全0初始化\"\"\"\n",
    "    print(\"=== 全0初始化 ===\")\n",
    "    linear = nn.Linear(5, 3)\n",
    "    nn.init.zeros_(linear.weight)\n",
    "    nn.init.zeros_(linear.bias)\n",
    "    print(f\"权重均值: {linear.weight.data.mean():.4f}\")\n",
    "    print(f\"权重:\\n{linear.weight.data}\\n\")\n",
    "\n",
    "def demo_ones_init():\n",
    "    \"\"\"演示全1初始化\"\"\"\n",
    "    print(\"=== 全1初始化 ===\")\n",
    "    linear = nn.Linear(5, 3)\n",
    "    nn.init.ones_(linear.weight)\n",
    "    nn.init.ones_(linear.bias)\n",
    "    print(f\"权重均值: {linear.weight.data.mean():.4f}\")\n",
    "    print(f\"权重:\\n{linear.weight.data}\\n\")\n",
    "\n",
    "def demo_kaiming_init():\n",
    "    \"\"\"演示Kaiming初始化（适合ReLU）\"\"\"\n",
    "    print(\"=== Kaiming初始化（适合ReLU） ===\")\n",
    "    linear = nn.Linear(5, 3)\n",
    "    nn.init.kaiming_uniform_(linear.weight, mode='fan_in', nonlinearity='relu')\n",
    "    nn.init.zeros_(linear.bias)\n",
    "    print(f\"权重标准差: {linear.weight.data.std():.4f}\")\n",
    "    print(f\"权重:\\n{linear.weight.data}\\n\")\n",
    "\n",
    "def demo_xavier_init():\n",
    "    \"\"\"演示Xavier初始化（适合Sigmoid/Tanh）\"\"\"\n",
    "    print(\"=== Xavier初始化（适合Sigmoid/Tanh） ===\")\n",
    "    linear = nn.Linear(5, 3)\n",
    "    nn.init.xavier_uniform_(linear.weight)\n",
    "    nn.init.zeros_(linear.bias)\n",
    "    print(f\"权重标准差: {linear.weight.data.std():.4f}\")\n",
    "    print(f\"权重:\\n{linear.weight.data}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    demo_uniform_init()\n",
    "    demo_zeros_init()\n",
    "    demo_ones_init()\n",
    "    demo_kaiming_init()\n",
    "    demo_xavier_init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe88ac7c4732854a",
   "metadata": {},
   "source": [
    "## 16. 神经网络搭建步骤\n",
    "\n",
    "### 搭建神经网络的三步骤：\n",
    "1. **定义类继承nn.Module**\n",
    "2. **在`__init__`中搭建神经网络层**\n",
    "3. **在`forward`中完成前向传播**\n",
    "\n",
    "### 深度学习案例的四个步骤：\n",
    "1. **准备数据**\n",
    "2. **搭建神经网络**\n",
    "3. **模型训练**\n",
    "4. **模型测试/预测**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8092ca9cafcaaa9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17. 神经网络模型搭建示例\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class ModelDemo(nn.Module):\n",
    "    \"\"\"\n",
    "    示例神经网络模型\n",
    "    结构: 3输入 → [3隐藏单元] → [2隐藏单元] → 2输出\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ModelDemo, self).__init__()\n",
    "        \n",
    "        # 搭建神经网络层\n",
    "        self.linear1 = nn.Linear(3, 3)  # 输入层 → 隐藏层1\n",
    "        self.linear2 = nn.Linear(3, 2)  # 隐藏层1 → 隐藏层2\n",
    "        self.linear3 = nn.Linear(2, 2)  # 隐藏层2 → 输出层\n",
    "        \n",
    "        # 参数初始化\n",
    "        nn.init.xavier_normal_(self.linear1.weight)  # Xavier适合Sigmoid\n",
    "        nn.init.zeros_(self.linear1.bias)\n",
    "        \n",
    "        nn.init.kaiming_normal_(self.linear2.weight)  # Kaiming适合ReLU\n",
    "        nn.init.zeros_(self.linear2.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"前向传播\"\"\"\n",
    "        # 第一层：线性变换 + Sigmoid激活\n",
    "        x = torch.sigmoid(self.linear1(x))\n",
    "        \n",
    "        # 第二层：线性变换 + ReLU激活\n",
    "        x = torch.relu(self.linear2(x))\n",
    "        \n",
    "        # 输出层：线性变换 + Softmax（用于多分类）\n",
    "        x = torch.softmax(self.linear3(x), dim=-1)  # dim=-1 按行计算\n",
    "        \n",
    "        return x\n",
    "\n",
    "def demo_model():\n",
    "    \"\"\"演示模型的使用\"\"\"\n",
    "    print(\"=== 模型结构 ===\")\n",
    "    \n",
    "    # 创建模型\n",
    "    model = ModelDemo()\n",
    "    \n",
    "    # 打印模型结构\n",
    "    print(model)\n",
    "    print()\n",
    "    \n",
    "    # 创建测试数据\n",
    "    data = torch.randn(size=(5, 3))\n",
    "    print(\"输入数据 (5个样本，3个特征):\")\n",
    "    print(data)\n",
    "    print()\n",
    "    \n",
    "    # 前向传播\n",
    "    output = model.forward(data)\n",
    "    print(\"模型输出 (5个样本，2个类别概率):\")\n",
    "    print(output)\n",
    "    print(f\"\\n输出总和（每行）: {output.sum(dim=1)}\")\n",
    "    print(f\"requires_grad: {output.requires_grad}\")\n",
    "    print()\n",
    "    \n",
    "    # 查看模型参数\n",
    "    print(\"=== 模型参数 ===\")\n",
    "    total_params = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"{name:20s} | 形状: {str(param.shape):15s} | 参数量: {param.numel()}\")\n",
    "        total_params += param.numel()\n",
    "    \n",
    "    print(f\"\\n总参数量: {total_params}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    demo_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78914bf2326bbcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18. 损失函数\n",
    "\n",
    "## 常用损失函数分类：\n",
    "\n",
    "### 分类问题：\n",
    "# - **多分类交叉熵损失**: `nn.CrossEntropyLoss()`（内置Softmax）\n",
    "# - **二分类交叉熵损失**: `nn.BCELoss()`（需要手动Sigmoid）\n",
    "\n",
    "# ### 回归问题：\n",
    "# - **MAE**: 平均绝对误差 `nn.L1Loss()`\n",
    "# - **MSE**: 均方误差 `nn.MSELoss()`\n",
    "# - **Smooth L1**: MAE和MSE的优化版 `nn.SmoothL1Loss()`\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def demo_bce_loss():\n",
    "    \"\"\"演示二分类交叉熵损失\"\"\"\n",
    "    print(\"=== 二分类交叉熵损失 (BCELoss) ===\")\n",
    "    \n",
    "    # 真实标签（需要是概率形式）\n",
    "    y_true = torch.tensor([0.0, 1.0, 0.0], dtype=torch.float32)\n",
    "    \n",
    "    # 预测概率（需要经过sigmoid，范围在[0,1]）\n",
    "    y_pred = torch.tensor([0.1, 0.8, 0.4], dtype=torch.float32)\n",
    "    \n",
    "    print(f\"真实标签: {y_true}\")\n",
    "    print(f\"预测概率: {y_pred}\")\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    loss = criterion(y_pred, y_true)\n",
    "    print(f\"损失值: {loss.item():.4f}\")\n",
    "\n",
    "def demo_mse_loss():\n",
    "    \"\"\"演示均方误差损失\"\"\"\n",
    "    print(\"\\n=== 均方误差损失 (MSELoss) ===\")\n",
    "    \n",
    "    y_true = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float32)\n",
    "    y_pred = torch.tensor([1.2, 1.8, 3.5], dtype=torch.float32)\n",
    "    \n",
    "    print(f\"真实值: {y_true}\")\n",
    "    print(f\"预测值: {y_pred}\")\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    loss = criterion(y_pred, y_true)\n",
    "    print(f\"损失值: {loss.item():.4f}\")\n",
    "\n",
    "def demo_smooth_l1_loss():\n",
    "    \"\"\"演示Smooth L1损失\"\"\"\n",
    "    print(\"\\n=== Smooth L1损失 ===\")\n",
    "    \n",
    "    y_true = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float32)\n",
    "    y_pred = torch.tensor([1.2, 1.8, 3.5], dtype=torch.float32)\n",
    "    \n",
    "    print(f\"真实值: {y_true}\")\n",
    "    print(f\"预测值: {y_pred}\")\n",
    "    \n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(y_pred, y_true)\n",
    "    print(f\"损失值: {loss.item():.4f}\")\n",
    "    print(\"\\n说明: Smooth L1在误差较小时使用平方误差，误差较大时使用绝对误差\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    demo_bce_loss()\n",
    "    demo_mse_loss()\n",
    "    demo_smooth_l1_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d27a6a7d5fd9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19. 网络优化方法对比\n",
    "\n",
    "## 梯度下降优化方法总结：\n",
    "\n",
    "### 1. **SGD (随机梯度下降)**\n",
    "   - 基础方法，每次只考虑当前梯度\n",
    "   - 公式: `W新 = W旧 - 学习率 × 梯度`\n",
    "\n",
    "### 2. **Momentum (动量法)**\n",
    "   - 考虑历史梯度信息\n",
    "   - 适合: 简单任务和小模型\n",
    "\n",
    "### 3. **AdaGrad (自适应学习率)**\n",
    "   - 根据梯度历史自动调整学习率\n",
    "   - 适合: 稀疏数据和文本数据\n",
    "\n",
    "### 4. **RMSprop**\n",
    "   - AdaGrad的改进版，加入衰减系数\n",
    "   - 适合: 稀疏数据和文本数据\n",
    "\n",
    "### 5. **Adam (自适应矩估计)**\n",
    "   - 结合了RMSprop和Momentum的优点\n",
    "   - 既优化学习率，又优化梯度\n",
    "   - **适合: 复杂任务和大量数据（推荐首选）**\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def demo_optimizer_comparison():\n",
    "    \"\"\"对比不同优化器\"\"\"\n",
    "    print(\"=== 优化器对比示例 ===\\n\")\n",
    "    \n",
    "    # 初始化权重\n",
    "    w_init = 1.0\n",
    "    print(f\"初始权重: {w_init}\")\n",
    "    print(f\"目标: 最小化 loss = w²/2\")\n",
    "    print(f\"理论最优解: w = 0\\n\")\n",
    "    \n",
    "    print(\"方法          | 第1次更新后 | 第2次更新后\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    # SGD\n",
    "    w = torch.tensor(w_init, requires_grad=True)\n",
    "    optimizer = optim.SGD([w], lr=0.01)\n",
    "    criterion = (w ** 2) / 2.0\n",
    "    optimizer.zero_grad()\n",
    "    criterion.backward()\n",
    "    optimizer.step()\n",
    "    result1 = w.item()\n",
    "    \n",
    "    criterion = (w ** 2) / 2.0\n",
    "    optimizer.zero_grad()\n",
    "    criterion.backward()\n",
    "    optimizer.step()\n",
    "    result2 = w.item()\n",
    "    print(f\"{'SGD':14s} | {result1:11.6f} | {result2:11.6f}\")\n",
    "    \n",
    "    # SGD + Momentum\n",
    "    w = torch.tensor(w_init, requires_grad=True)\n",
    "    optimizer = optim.SGD([w], lr=0.01, momentum=0.9)\n",
    "    criterion = (w ** 2) / 2.0\n",
    "    optimizer.zero_grad()\n",
    "    criterion.backward()\n",
    "    optimizer.step()\n",
    "    result1 = w.item()\n",
    "    \n",
    "    criterion = (w ** 2) / 2.0\n",
    "    optimizer.zero_grad()\n",
    "    criterion.backward()\n",
    "    optimizer.step()\n",
    "    result2 = w.item()\n",
    "    print(f\"{'Momentum':14s} | {result1:11.6f} | {result2:11.6f}\")\n",
    "    \n",
    "    # AdaGrad\n",
    "    w = torch.tensor(w_init, requires_grad=True)\n",
    "    optimizer = optim.Adagrad([w], lr=0.01)\n",
    "    criterion = (w ** 2) / 2.0\n",
    "    optimizer.zero_grad()\n",
    "    criterion.backward()\n",
    "    optimizer.step()\n",
    "    result1 = w.item()\n",
    "    \n",
    "    criterion = (w ** 2) / 2.0\n",
    "    optimizer.zero_grad()\n",
    "    criterion.backward()\n",
    "    optimizer.step()\n",
    "    result2 = w.item()\n",
    "    print(f\"{'AdaGrad':14s} | {result1:11.6f} | {result2:11.6f}\")\n",
    "    \n",
    "    # RMSprop\n",
    "    w = torch.tensor(w_init, requires_grad=True)\n",
    "    optimizer = optim.RMSprop([w], lr=0.01, alpha=0.99)\n",
    "    criterion = (w ** 2) / 2.0\n",
    "    optimizer.zero_grad()\n",
    "    criterion.backward()\n",
    "    optimizer.step()\n",
    "    result1 = w.item()\n",
    "    \n",
    "    criterion = (w ** 2) / 2.0\n",
    "    optimizer.zero_grad()\n",
    "    criterion.backward()\n",
    "    optimizer.step()\n",
    "    result2 = w.item()\n",
    "    print(f\"{'RMSprop':14s} | {result1:11.6f} | {result2:11.6f}\")\n",
    "    \n",
    "    # Adam\n",
    "    w = torch.tensor(w_init, requires_grad=True)\n",
    "    optimizer = optim.Adam([w], lr=0.01, betas=(0.9, 0.999))\n",
    "    criterion = (w ** 2) / 2.0\n",
    "    optimizer.zero_grad()\n",
    "    criterion.backward()\n",
    "    optimizer.step()\n",
    "    result1 = w.item()\n",
    "    \n",
    "    criterion = (w ** 2) / 2.0\n",
    "    optimizer.zero_grad()\n",
    "    criterion.backward()\n",
    "    optimizer.step()\n",
    "    result2 = w.item()\n",
    "    print(f\"{'Adam':14s} | {result1:11.6f} | {result2:11.6f}\")\n",
    "    \n",
    "    print(\"\\n结论: Adam收敛最快，Momentum次之，SGD最慢\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    demo_optimizer_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8744a330ea71119",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T10:28:41.733129Z",
     "start_time": "2026-01-27T10:28:41.576410Z"
    }
   },
   "outputs": [],
   "source": [
    "# 20. 指数加权平均与动量法\n",
    "\n",
    "## 指数加权平均 (Exponentially Weighted Averages)\n",
    "\n",
    "指数加权平均是一种时间序列平滑方法，在深度学习中用于：\n",
    "- **动量法**: 平滑梯度更新\n",
    "- **RMSprop/Adam**: 自适应学习率\n",
    "\n",
    "**公式:** `Vt = β × Vt-1 + (1-β) × Xt`\n",
    "\n",
    "- β 越大，数据越平缓，历史权重越大\n",
    "- β 越小，数据越陡，当前值权重越大\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 设置图形样式\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rcParams['figure.figsize'] = (15, 4)\n",
    "\n",
    "ELEMENT_NUMBER = 30\n",
    "\n",
    "def demo_raw_temperature():\n",
    "    \"\"\"演示原始温度数据\"\"\"\n",
    "    torch.manual_seed(0)\n",
    "    temperature = torch.randn(ELEMENT_NUMBER) * 10 + 20\n",
    "    \n",
    "    days = torch.arange(1, ELEMENT_NUMBER + 1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(days, temperature, 'o-', color='#E74C3C', linewidth=2, markersize=6)\n",
    "    plt.title('原始温度数据（30天）', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('天数', fontsize=12)\n",
    "    plt.ylabel('温度 (°C)', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return temperature\n",
    "\n",
    "def demo_exponential_weighted_average(beta=0.9):\n",
    "    \"\"\"演示指数加权平均\"\"\"\n",
    "    torch.manual_seed(0)\n",
    "    temperature = torch.randn(ELEMENT_NUMBER) * 10 + 20\n",
    "    \n",
    "    # 计算指数加权平均\n",
    "    exp_weight_avg = []\n",
    "    for idx, temp in enumerate(temperature, 1):\n",
    "        if idx == 1:\n",
    "            exp_weight_avg.append(temp)\n",
    "        else:\n",
    "            new_temp = exp_weight_avg[idx - 2] * beta + (1 - beta) * temp\n",
    "            exp_weight_avg.append(new_temp)\n",
    "    \n",
    "    exp_weight_avg = torch.tensor(exp_weight_avg)\n",
    "    days = torch.arange(1, ELEMENT_NUMBER + 1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(days, exp_weight_avg, 'o-', color='#3498DB', \n",
    "             linewidth=2, markersize=6, label=f'指数加权平均 (β={beta})')\n",
    "    plt.scatter(days, temperature, color='#E74C3C', alpha=0.3, s=30, label='原始数据')\n",
    "    plt.title(f'指数加权平均平滑效果 (β={beta})', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('天数', fontsize=12)\n",
    "    plt.ylabel('温度 (°C)', fontsize=12)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"=== 指数加权平均演示 ===\\n\")\n",
    "    \n",
    "    # 原始数据\n",
    "    print(\"1. 原始温度数据:\")\n",
    "    temp = demo_raw_temperature()\n",
    "    \n",
    "    # 不同β值的影响\n",
    "    betas = [0.5, 0.9, 0.98]\n",
    "    for beta in betas:\n",
    "        print(f\"\\n2. β = {beta} 时的平滑效果:\")\n",
    "        demo_exponential_weighted_average(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a604583055a2a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 动量法 (Momentum) ===\n",
      "第1次迭代 | w: 0.990000, w.grad: 1.000000\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[3], line 63\u001b[0m\n",
      "\u001b[0;32m     60\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m第\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m次迭代 | w: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, w.grad: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[1;32m---> 63\u001b[0m     \u001b[43mdemo_momentum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m     64\u001b[0m     demo_adagrad()\n",
      "\u001b[0;32m     65\u001b[0m     demo_rmsprop()\n",
      "\n",
      "Cell \u001b[1;32mIn[3], line 16\u001b[0m, in \u001b[0;36mdemo_momentum\u001b[1;34m()\u001b[0m\n",
      "\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m):\n",
      "\u001b[0;32m     15\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[1;32m---> 16\u001b[0m     \u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m     17\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m第\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m次迭代 | w: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, w.grad: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mw\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "File \u001b[1;32mD:\\tools\\XFC\\anaconda\\envs\\PythonProject8\\Lib\\site-packages\\torch\\_tensor.py:629\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n",
      "\u001b[0;32m    619\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[0;32m    620\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n",
      "\u001b[0;32m    621\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n",
      "\u001b[0;32m    622\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    627\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n",
      "\u001b[0;32m    628\u001b[0m     )\n",
      "\u001b[1;32m--> 629\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m    630\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n",
      "\u001b[0;32m    631\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32mD:\\tools\\XFC\\anaconda\\envs\\PythonProject8\\Lib\\site-packages\\torch\\autograd\\__init__.py:364\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n",
      "\u001b[0;32m    359\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "\u001b[0;32m    361\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n",
      "\u001b[0;32m    362\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n",
      "\u001b[0;32m    363\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n",
      "\u001b[1;32m--> 364\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    370\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    372\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32mD:\\tools\\XFC\\anaconda\\envs\\PythonProject8\\Lib\\site-packages\\torch\\autograd\\graph.py:865\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    863\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n",
      "\u001b[0;32m    864\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m--> 865\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n",
      "\u001b[0;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n",
      "\u001b[0;32m    867\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n",
      "\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;32m    869\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "# 21. 完整优化器代码参考\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def demo_momentum():\n",
    "    \"\"\"动量法优化器\"\"\"\n",
    "    print(\"=== 动量法 (Momentum) ===\")\n",
    "    w = torch.tensor([1.0], requires_grad=True, dtype=torch.float32)\n",
    "    criterion = (w ** 2) / 2.0\n",
    "    optimizer = optim.SGD([w], lr=0.01, momentum=0.9)\n",
    "    \n",
    "    for i in range(2):\n",
    "        optimizer.zero_grad()\n",
    "        criterion.sum().backward()\n",
    "        optimizer.step()\n",
    "        print(f\"第{i+1}次迭代 | w: {w.item():.6f}, w.grad: {w.grad.item():.6f}\")\n",
    "    print()\n",
    "\n",
    "def demo_adagrad():\n",
    "    \"\"\"AdaGrad优化器\"\"\"\n",
    "    print(\"=== AdaGrad ===\")\n",
    "    w = torch.tensor([1.0], requires_grad=True, dtype=torch.float32)\n",
    "    criterion = (w ** 2) / 2.0\n",
    "    optimizer = optim.Adagrad([w], lr=0.01)\n",
    "    \n",
    "    for i in range(2):\n",
    "        optimizer.zero_grad()\n",
    "        criterion.sum().backward()\n",
    "        optimizer.step()\n",
    "        print(f\"第{i+1}次迭代 | w: {w.item():.6f}, w.grad: {w.grad.item():.6f}\")\n",
    "    print()\n",
    "\n",
    "def demo_rmsprop():\n",
    "    \"\"\"RMSprop优化器\"\"\"\n",
    "    print(\"=== RMSprop ===\")\n",
    "    w = torch.tensor([1.0], requires_grad=True, dtype=torch.float32)\n",
    "    criterion = (w ** 2) / 2.0\n",
    "    optimizer = optim.RMSprop([w], lr=0.01, alpha=0.99)\n",
    "    \n",
    "    for i in range(2):\n",
    "        optimizer.zero_grad()\n",
    "        criterion.sum().backward()\n",
    "        optimizer.step()\n",
    "        print(f\"第{i+1}次迭代 | w: {w.item():.6f}, w.grad: {w.grad.item():.6f}\")\n",
    "    print()\n",
    "\n",
    "def demo_adam():\n",
    "    \"\"\"Adam优化器\"\"\"\n",
    "    print(\"=== Adam ===\")\n",
    "    w = torch.tensor([1.0], requires_grad=True, dtype=torch.float32)\n",
    "    criterion = (w ** 2) / 2.0\n",
    "    optimizer = optim.Adam([w], lr=0.01, betas=(0.9, 0.999))\n",
    "    \n",
    "    for i in range(2):\n",
    "        optimizer.zero_grad()\n",
    "        criterion.sum().backward()\n",
    "        optimizer.step()\n",
    "        print(f\"第{i+1}次迭代 | w: {w.item():.6f}, w.grad: {w.grad.item():.6f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    demo_momentum()\n",
    "    demo_adagrad()\n",
    "    demo_rmsprop()\n",
    "    demo_adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb205ae8c5674c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 22. 学习总结\n",
    "#\n",
    "# ## PyTorch深度学习核心知识点回顾\n",
    "#\n",
    "# ### 1. 张量 (Tensor)\n",
    "# - ✅ 创建方法: `tensor()`, `ones()`, `zeros()`, `randn()`, `arange()`, `linspace()`\n",
    "# - ✅ 类型转换: `.type()`, `.float()`, `.long()`\n",
    "# - ✅ 与NumPy互转: `.numpy()`, `from_numpy()`\n",
    "#\n",
    "# ### 2. 张量操作\n",
    "# - ✅ 算术运算: 加减乘除、点乘、矩阵乘法\n",
    "# - ✅ 索引: 基础索引、高级索引、条件索引\n",
    "# - ✅ 形状操作: `reshape()`, `unsqueeze()`, `squeeze()`, `transpose()`\n",
    "#\n",
    "# ### 3. 自动微分\n",
    "# - ✅ `requires_grad=True`: 标记需要计算梯度的张量\n",
    "# - ✅ `.backward()`: 反向传播计算梯度\n",
    "# - ✅ `.grad`: 获取梯度值\n",
    "# - ✅ `.detach()`: 从计算图中分离\n",
    "#\n",
    "# ### 4. 神经网络\n",
    "# - ✅ 参数初始化: Xavier, Kaiming\n",
    "# - ✅ 搭建模型: 继承`nn.Module`\n",
    "# - ✅ 前向传播: `forward()`方法\n",
    "#\n",
    "# ### 5. 损失函数\n",
    "# - ✅ 分类: `CrossEntropyLoss`, `BCELoss`\n",
    "# - ✅ 回归: `MSELoss`, `L1Loss`, `SmoothL1Loss`\n",
    "#\n",
    "# ### 6. 优化器\n",
    "# - ✅ SGD: 基础梯度下降\n",
    "# - ✅ Momentum: 加入动量\n",
    "# - ✅ AdaGrad: 自适应学习率\n",
    "# - ✅ RMSprop: AdaGrad改进版\n",
    "# - ✅ Adam: 推荐首选（综合RMSprop+Momentum）\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cd70557364aa7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_list: [[0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkAAAAGwCAYAAABB4NqyAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAMkJJREFUeJzt3Ql4VNXdx/F/dkgkYUkF2UFQQBCQJcWqaKWCUBG1irw8BSmFggooSBVkqVqLVUG08Iq8Ral1AbFCXRDKXhUoexVZ3AFBdgkQIIHMfZ//MTNmIMBMMsnMvef7eZ55krlzM7l3bu7ML+f8z7lxjuM4AgAAYJH4aG8AAABAWSMAAQAA6xCAAACAdQhAAADAOgQgAABgHQIQAACwDgEIAABYJzHaGxCLfD6f7Nq1SypUqCBxcXHR3hwAABACndrwyJEjUr16dYmPP3cbDwGoCBp+atWqFcprDQAAYsyOHTukZs2a51yHAFQEbfnxv4Dp6emlc3QAAEBEHT582DRg+D/Hz4UAVAR/t5eGHwIQAADuEkr5CkXQAADAOgQgAABgHQIQAACwDgEIAABYhwAEAACsQwACAADWIQABAADrEIAAAIB1CEAAAMA6BCAAAGCdqAegyZMnS926daVcuXKSlZUlq1atOuu6n376qdx2221mfZ3meuLEiSV+TgAAYJ+oBqCZM2fK0KFDZezYsbJu3Tpp3ry5dOzYUfbu3Vvk+seOHZP69evLE088IdWqVYvIcwIAAPvEOY7jROuXa+tMmzZtZNKkSea+z+czV3EdNGiQPPTQQ+f8WW3hue+++8wtUs9Z+GqyGRkZkp2dHdGLoR4+cVIOHz8ZsedD2UpOjJcLK5TjZQeAGBXO53fUrgafl5cna9eulREjRgSWxcfHS4cOHWTFihVl+py5ubnmVvgFLA2vrNwmT87bWirPjbIx+pdNpO9V9Xi5AcDlohaA9u/fL/n5+VK1atWg5Xp/y5YtZfqc48aNk0ceeURKW2J8nKQkRr3sCsWQ73PklM+RDTsO8foBgAdELQDFEm0x0rqhwi1A2m0Waf2vudjc4D7TP/pa/vDOJvH5otZjDADwQgDKzMyUhIQE2bNnT9ByvX+2AufSes6UlBRzA84mIT4u0BIEAHC/qPXHJCcnS6tWrWTRokWBZVqwrPfbtWsXM88JqHh/AIremAEAgFe6wLTbqXfv3tK6dWtp27atmdcnJydH+vTpYx7v1auX1KhRw9To+IucN23aFPh+586dsmHDBrngggukQYMGIT0nUBwJcT8EILrAAMAbohqAunfvLvv27ZMxY8bI7t27pUWLFjJv3rxAEfP27dvNKC6/Xbt2ScuWLQP3n376aXNr3769LF26NKTnBIqDFiAA8JaozgMUq0prHiC41z/WfivDZv1Xrm6YKX/vmxXtzQEAlPDzmzHZQBhF0D7+XwAATyAAAaGcKIwCAwBPIQABYRVB83IBgBcQgIAQJBScKQyDBwBvIAABoZwoBS1ATIQIAN5AAAJCQBE0AHgLAQgI5UShCBoAPIUABIRRBE0XGAB4AwEICAFdYADgLQQgIJQThRYgAPAUAhAQVgsQLxcAeAEBCAhnHiASEAB4AgEICOVEoQsMADyFAASEgCJoAPAWAhAQyolCCxAAeAoBCAgBLUAA4C0EICCMAEQRNAB4AwEICOVEoQsMADyFAASEgHmAAMBbCEBACLgWGAB4CwEICOVE8U+E6DAVNAB4AQEICKcLjJmgAcATCEBAOF1gtAABgCcQgIBQTpSCFiDNPw4hCABcjwAEhNECpJgLCADcjwAEhNECpOgGAwD3IwABYRRBK5+PlwwA3I4ABITbBUYNEAC4HgEICOVEKXSmUAMEAO5HAALCbAFiLiAAcD8CEBBmDRBdYADgfgQgIARxcXHibwSiBQgA3I8ABISI2aABwDsIQECoJ0tBNxhF0ADgfgQgIMwWIOYBAgD3IwABIUr0twAxDxAAuB4BCAj1ZKELDAA8gwAEhDkU3kcLEAC4HgEICPVkKagBOpXv8JoBgMsRgIAQJRScLbQAAYD7EYCAcOcB8tECBABuRwACQj1ZGAUGAJ5BAALCLYKmBQgAXI8ABISILjAA8A4CEBDqyUIXGAB4BgEICBGXwgAA7yAAAaGeLLQAAYBnEICAcOcBoggaAFyPAASEiCJoAPAOAhAQ6slCFxgAeAYBCAi7CJqZoAHA7QhAQKgnCy1AAOAZBCAgRNQAAYB3EICAcC+F4dAFBgBuRwACwu0C8/GSAYDbEYCAECX8kH8oggYADyAAAWF2geXTBQYArkcAAkI9WQqGweczDB4AXI8ABISIImgA8A4CEBB2ETSjwADA7QhAQIiYBwgAvIMABISILjAA8A4CEBB2ETQvGQC4HQEICFFCwdnCTNAA4H4EICDceYAoggYA1yMAAaGeLMwDBACeEfUANHnyZKlbt66UK1dOsrKyZNWqVedcf9asWdKoUSOzfrNmzWTu3LlBjx89elTuvfdeqVmzppQvX16aNGkiU6ZMKeW9gA0oggYA74hqAJo5c6YMHTpUxo4dK+vWrZPmzZtLx44dZe/evUWuv3z5cunRo4f07dtX1q9fL926dTO3jRs3BtbR55s3b5688sorsnnzZrnvvvtMIHr77bfLcM/gRbQAAYB3RDUATZgwQfr16yd9+vQJtNSkpqbKiy++WOT6zz77rHTq1EmGDx8ujRs3lscee0yuuOIKmTRpUlBI6t27t1x77bWmZal///4mWJ2vZQk4H64FBgDeEbUAlJeXJ2vXrpUOHTr8uDHx8eb+ihUrivwZXV54faUtRoXXv/LKK01rz86dO8VxHFmyZIl89tlncsMNN5x1W3Jzc+Xw4cNBN+CsXWAUQQOA60UtAO3fv1/y8/OlatWqQcv1/u7du4v8GV1+vvX/8pe/mNYkrQFKTk42LUZaZ3TNNdecdVvGjRsnGRkZgVutWrVKvH/wHuYBAgDviHoRdKRpAFq5cqVpBdIWpvHjx8s999wjCxcuPOvPjBgxQrKzswO3HTt2lOk2wx2YBwgAvCMxWr84MzNTEhISZM+ePUHL9X61atWK/Bldfq71jx8/LiNHjpTZs2dLly5dzLLLL79cNmzYIE8//fQZ3Wd+KSkp5gacC9cCAwDviFoLkHZPtWrVShYtWhRY5vP5zP127doV+TO6vPD6asGCBYH1T548aW5aS1SYBi19biAiV4N3uBo8ALhd1FqA/EPWdcRW69atpW3btjJx4kTJyckxo8JUr169pEaNGqZGRw0ZMkTat29vurW0hWfGjBmyZs0amTp1qnk8PT3dPK6jxHQOoDp16siyZcvk5ZdfNiPOgEi0AFEEDQDuF9UA1L17d9m3b5+MGTPGFDK3aNHCzOHjL3Tevn17UGuOjvB67bXXZNSoUaarq2HDhjJnzhxp2rRpYB0NRVrT07NnTzl48KAJQY8//rgMGDAgKvsID7YAMQoMAFwvztGx4giiw+B1NJgWRGurEqCmLPtSnnh/i9x6RQ2ZcEcLXhQAcPHnt+dGgQGlhS4wAPAOAhAQdhE0LxkAuB0BCAhRwg/5hyJoAPAAAhAQ7rXAKIIGANcjAAGhnizMAwQAnkEAAkKUyMVQAcAzCEBAqCdLwUSIzAQNAO5HAAJCRA0QAHgHAQgIMwD5mDsUAFyPAASE2wXGKDAAcD0CEBBuC5CPlwwA3I4ABIR6slAEDQCeQQACQkQRNAB4BwEICFFCwdlCETQAuB8BCAj1ZKEIGgA8gwAEhIguMADwDgIQEKKEghYgusAAwP0IQECoJwtXgwcAzyAAAWHPBM1LBgBuRwACQj1ZKIIGAM8gAAEhoggaALyDAASEiCJoAPAOAhAQ6slScLZwMVQAcD8CEBAiusAAwDsIQECYXWD5DsPAAMDtCEBAqCcL8wABgGcQgIBwi6CZCAgAXI8ABIRbA0QXGAC4HgEICPVk8c8E7eMlAwC3IwABIaIIGgC8gwAEhHqyMA8QAHgGAQgIswVIUQgNAO5GAALCLIJWFEIDgLsRgIBQT5bCAYih8ADgagQgoDhdYAyFBwBXIwABxekCowUIAFyNAASEerIEFUHzsgGAtQHoxIkTkdsSIMZRBA0AFgcgn88njz32mNSoUUMuuOAC+eqrr8zy0aNHy7Rp00pjG4GYUKgHjC4wALAtAP3xj3+U6dOny5NPPinJycmB5U2bNpW//vWvkd4+IGbExcUFQhBF0ABgWQB6+eWXZerUqdKzZ09JSEgILG/evLls2bIl0tsHxOYFUSmCBgC7AtDOnTulQYMGRXaNnTx5MlLbBcR0ITQBCAAsC0BNmjSRDz744Izlb775prRs2TJS2wXEdAsQXWAA4G6J4f7AmDFjpHfv3qYlSFt93nrrLdm6davpGnv33XdLZyuBWLsiPF1gAGBXC9DNN98s77zzjixcuFDS0tJMINq8ebNZ9otf/KJ0thKIscth0AIEAJa1AKmrr75aFixYEPmtAVxTBB3tLQEAlGkLUP369eXAgQNnLD906JB5DPAyiqABwNIA9M0330h+fv4Zy3Nzc01dEOBlCQVnDF1gAGBJF9jbb78d+H7+/PmSkZERuK+BaNGiRVK3bt3IbyEQQyiCBgDLAlC3bt0Cs+HqKLDCkpKSTPgZP3585LcQiMEi6HzHifamAADKIgDpkHdVr149Wb16tWRmZpbk9wLungeIYfAAYNcosK+//rp0tgRwAS6FAQAWD4PPycmRZcuWyfbt2yUvLy/oscGDB0dq24DYrQGiCwwA7ApA69evl86dO8uxY8dMEKpcubLs379fUlNT5cILLyQAwZIusGhvCQCgTIfB33///XLTTTfJ999/L+XLl5eVK1fKtm3bpFWrVvL000+XaGMA18wDRAsQANgVgDZs2CDDhg2T+Ph4SUhIMPP/1KpVS5588kkZOXJk6WwlECMoggYASwOQDnnX8KO0y0vrgJTOC7Rjx47IbyEQi8PgGQUGAHbVALVs2dIMg2/YsKG0b9/eXAxVa4D+/ve/S9OmTUtnK4EYkfBD/qELDABsawH605/+JBdddJH5/vHHH5dKlSrJwIEDZd++ffLCCy+UxjYCMYMuMACwtAWodevWge+1C2zevHmR3iYgZlEEDQCWtgCdzbp16+SXv/xlpJ4OiElMhAgAFgYgvQjqAw88YEZ7ffXVV2bZli1bzHXC2rRpE7hcBuD5LjCGwQOAHV1g06ZNk379+pmJD3UOoL/+9a8yYcIEGTRokHTv3l02btwojRs3Lt2tBWKlC4ysDwB2tAA9++yz8uc//9mM+HrjjTfM1//93/+VTz75RKZMmUL4gRUoggYAywLQl19+Kbfffrv5/tZbb5XExER56qmnpGbNmqW5fUBMoQgaACwLQMePHzfX+1JxcXGSkpISGA4P2CKh4IxhIkQAsKgIWut+nnvuOXM7deqUTJ8+PXDffwvX5MmTpW7dulKuXDnJysqSVatWnXP9WbNmSaNGjcz6zZo1k7lz556xzubNm6Vr165mduq0tDRToO2fsRooCYqgAcCyIujatWvL//3f/wXuV6tWzcz+XJi2DA0ePDjkXz5z5kwZOnSoqSHS8DNx4kTp2LGjbN261cwxdLrly5dLjx49ZNy4cWbI/WuvvWZGoOkQfP8s1NpVd9VVV0nfvn3lkUcekfT0dPn0009NYAIiVwTt8GICgIvFOU70xvNq6NHWmUmTJpn7OoxeL6yqI8seeuihM9bX0WY5OTny7rvvBpb99Kc/lRYtWpgQpe68805zvbLTw1k4Dh8+bFqPsrOzTYAC/IbMWC//3LBLRnVpLL+9uj4vDADEkHA+vyM2EWK48vLyZO3atdKhQ4cfNyY+3txfsWJFkT+jywuvr7TFyL++Bqj33ntPLrnkErNcW5E0ZM2ZM+ec26JXtNcXrfANKEpCQQsQ8wABgLtFLQDpMPr8/HypWrVq0HK9v3v37iJ/Rpefa/29e/fK0aNH5YknnpBOnTrJv/71L7nlllvMqLVly5addVu0S00To/+mrVDAua8Gz+sDAG4WtQBUGvwzUd98881y//33m64x7UrTeiF/F1lRRowYYZrL/LcdO3aU4VbDTWgBAgBLL4YaKZmZmZKQkCB79uwJWq73tcC6KLr8XOvrc+r8RE2aNAlaR2eo/vDDD8+6LTqkX29A6C1AFEEDgJtFrQUoOTlZWrVqJYsWLQpqwdH77dq1K/JndHnh9dWCBQsC6+tzalG1jiIr7LPPPpM6deqUyn7ALswDBACWtgCdrUDYPzmihpBQ6RD43r17S+vWraVt27ZmGLyO8urTp495vFevXlKjRg1To6OGDBki7du3l/Hjx0uXLl1kxowZsmbNGpk6dWrgOYcPH25Gi11zzTVy3XXXybx58+Sdd96RpUuXhrurwBnoAgMASwNQxYoVTdg5G700xl133SVjx441o7rORYPKvn37ZMyYMaaQWWt2NLD4C5118sLCz3HllVeauX9GjRplrkjfsGFDM8LLPweQ0qJnrffR0KRzEl166aXyj3/8w8wNBJQUXWAAYOk8QC+//LI8/PDDJuRoq43S2Zv/9re/mWCigebpp582LTEaUtyIeYBwNn98d5P89cOv5Xft68uIGxvzQgGASz+/w24B0qCjXVB33HFHYNlNN91kLkvxwgsvmBodnTX68ccfd20AAs6Gq8EDgKVF0Ho5ipYtW56xXJf5JyTU7iauvQUvYh4gALA0AOkkgdOmTTtjuS7zTyB44MABqVSpUmS2EIghFEEDgDeE3QWm9T233367vP/++2bIudKRWFu2bJE333zT3F+9erUpcAa8hiJoALA0AHXt2tWEHa330fl11I033mhGY9WtW9fcHzhwYOS3FIihFqD86F1DGAAQrZmg69WrZ663Bdg6EaKPmaABwL4AdOjQITP0XS8+6r/+lp9OXgh4FV1gAGBpANJZlXv27Gmuuq5j7AtPiqjfE4DgZXSBAYClo8CGDRsmv/nNb0wA0pag77//PnA7ePBg6WwlEGPzAHExVACwLADt3LnTXGIiNTW1dLYIiGHx/iJoaoAAwK4A1LFjRzPsHbB6JmhGgQGAXTVAehV2vc7Xpk2bzOUvkpKSzhgmD3gVRdAAYGkA6tevn/n66KOPnvGYFkHn5+dHZsuAWC6CDh78CADwegA6fdg7YOU8QHSBAYBdNUCAzSiCBgCLWoCee+456d+/v5QrV858fy46QgzwKoqgAcCiAPTMM8+YyQ81AOn3Z6M1QAQgeBnzAAGARQHo66+/LvJ7wDZ0gQGAN1ADBISBLjAAsHQUmA5znz59uixatKjIi6EuXrw4ktsHxBRagADA0gA0ZMgQE4B0QsSmTZsGXQwVsKYGyIn2lgAAyjQAzZgxQ9544w3p3LlziX4x4EaJ/kthcC0wALCrBig5OVkaNGhQOlsDxDguhQEAlgagYcOGybPPPisOM+HC4kthMBM0AFjWBfbhhx/KkiVL5P3335fLLrvsjIuhvvXWW5HcPiCmxBf8y5BPFxgA2BWAKlasKLfcckvpbA3glouh0gIKAPYEoFOnTsl1110nN9xwg1SrVq30tgqI9XmAaAECAHtqgBITE2XAgAGSm5tbelsEuKEImhYgALCrCLpt27ayfv360tkawC1F0MHzfwIAvF4DdPfdd5uRYN9++620atVK0tLSgh6//PLLI7l9QEx2geXl++Tb749Fe3NQDNUzygda8gDYK84Jczx7vH8YTOEniYszw+L1q14qw+0OHz4sGRkZkp2dLenp6dHeHMSQTbsOS+fnPoj2ZqAErmqQKa/8NovXEPCgcD6/w24B4mrwsNnFF6ZJsxoZ8tmeI9HeFIRJ/9XTlrv127/ntQMQfgCqU6cOLxuslZKYIO8Muiram4Fi+C77uLQbt1hOciE3AMVpAfLbtGmTbN++XfLy8oKWd+3alRcWQMxJSvih+15bgfxd9gDsFXYA+uqrr8xEiJ988kmg9kf530y8UAMEwLsBSJ3yOZKUQAACbBb2MPghQ4ZIvXr1ZO/evZKamiqffvqp/Pvf/5bWrVvL0qVLS2crAaCEkgsFoJP5zGMA2C7sFqAVK1bI4sWLJTMz04wI09tVV10l48aNk8GDBzNHEICYVLjF5+QpRyQ5qpsDwG0tQNrFVaFCBfO9hqBdu3YFiqO3bt0a+S0EgAjN4eQv+9E6IAB2C7sFqGnTpvLf//7XdINlZWXJk08+KcnJyTJ16lSpX79+6WwlAJSQ1ilqHVDeKR9dYADCD0CjRo2SnJwc8/2jjz4qv/zlL+Xqq6+WKlWqyMyZM3lJAcR0HZAGIL0BsFvYAahjx46B7xs0aCBbtmyRgwcPSqVKlRhWCsAVdUAUQQMIuwbI74svvpD58+fL8ePHpXLlyrySAFw1FxAAu4UdgA4cOCDXX3+9XHLJJdK5c2f57rvvzPK+ffuai6QCQKwHIGaDBhB2ALr//vslKSnJzAKt8wD5de/eXebNm8crCiBmJSf6AxAtQIDtwq4B+te//mW6vmrWrBm0vGHDhrJt27ZIbhsAlE4NEEXQgPXCbgHSEWCFW378tBA6JSXF+hcUQOyiBghAsQOQDnl/+eWXg+bW8Pl8Zj6g6667LtynA4AyQw0QgGJ3gWnQ0SLoNWvWmCvB//73vzfXA9MWoI8++ijcpwOAMr8eGDVAAOKLMxP0Z599Zq7/dfPNN5susVtvvdVcA+ziiy/mFQUQs5ISmQcIQDFbgFRGRoY8/PDDQcu+/fZb6d+/v7kkBgDEdA0QRdCA9Yo9EWJR8wNNmzbN+hcUQOyiBghAxAMQAMQ65gEC4EcAAmANiqAB+BGAAFg3ESLXAgMQchG0jvQ6l0OHDvFqAnBHDdApJ9qbAsAtAUhHfp3v8V69ekVimwCglIuguRYYYLuQA9BLL71UulsCAKWMImgAftQAAbAGNUAA/AhAAKxBFxgAPwIQAGtQBA3AjwAEwBrMAwTAjwAEwBrUAAHwIwABsEZSIhdDBfADAhAAa1AEDcCPAATAwhogZoIGbEcAAmBdCxDXAgMQEwFo8uTJUrduXSlXrpxkZWXJqlWrzrn+rFmzpFGjRmb9Zs2aydy5c8+67oABAyQuLk4mTpxYClsOwI1F0FwKA0DUA9DMmTNl6NChMnbsWFm3bp00b95cOnbsKHv37i1y/eXLl0uPHj2kb9++sn79eunWrZu5bdy48Yx1Z8+eLStXrpTq1auXwZ4AcEsRNAEIQNQD0IQJE6Rfv37Sp08fadKkiUyZMkVSU1PlxRdfLHL9Z599Vjp16iTDhw+Xxo0by2OPPSZXXHGFTJo0KWi9nTt3yqBBg+TVV1+VpKSkc25Dbm6uHD58OOgGwMM1QFwNHrBeVANQXl6erF27Vjp06BBYFh8fb+6vWLGiyJ/R5YXXV9piVHh9n88nv/71r01Iuuyyy867HePGjTNXs/ffatWqVaL9AhCbGAUGICYC0P79+yU/P1+qVq0atFzv7969u8if0eXnW//Pf/6zJCYmyuDBg0PajhEjRkh2dnbgtmPHjmLtD4DYxkSIAPwSxWO0RUm7ybSeSIufQ5GSkmJuALyNFiAAMdEClJmZKQkJCbJnz56g5Xq/WrVqRf6MLj/X+h988IEpoK5du7ZpBdLbtm3bZNiwYWakGQB7JQeKoJkHCLBdVANQcnKytGrVShYtWhRUv6P327VrV+TP6PLC66sFCxYE1tfan48//lg2bNgQuOkoMK0Hmj9/finvEQB3XA3eF+1NAWB7F5gOge/du7e0bt1a2rZta+brycnJMaPCVK9evaRGjRqmUFkNGTJE2rdvL+PHj5cuXbrIjBkzZM2aNTJ16lTzeJUqVcytMB0Fpi1El156aRT2EECstQAxESKAqAeg7t27y759+2TMmDGmkLlFixYyb968QKHz9u3bzcgwvyuvvFJee+01GTVqlIwcOVIaNmwoc+bMkaZNm0ZxLwC4ARMhAvCLcxyHzvDT6DxAOhxeR4Slp6ef/jAAlzp0LE9aPLrAfP/lnzpLQnxoAyUAeO/zO+oTIQJAWdcAKWaDBuxGAAJgZQCiDgiwGwEIgHU1QIqRYIDdCEAArKGTo/5YCE35I2AzAhAAqzAbNABFAAJgZQCiBgiwGwEIgFVoAQKgCEAArJLsrwE6RQ0QYDMCEACrJHE5DAAEIADW1gBxQVTAarQAAbAKNUAAFAEIgJ01QPm+aG8KgCgiAAGwCi1AABQBCICl8wAxCgywGQEIgJWjwLgWGGA3AhAAq1ADBEARgABYhRogAIoABMAq1AABUAQgAFahBQiAIgABsEpyov9aYMwDBNiMAATAKskFw+CZCBGwGwEIgFWoAQKgCEAA7JwHiEthAFYjAAGwCkXQABQBCIBVmAgRgCIAAbCzBugU1wIDbEYAAmAVusAAKAIQAKtQBA1AEYAAWIUaIACKAATAKswDBEARgADYWQPEpTAAqxGAAFiFImgAigAEwMqLoeYxEzRgNQIQAEvnAeJq8IDNCEAArEIXGABFAAJgaQBiJmjAZgQgAFZJDgQgusAAmxGAAFglqaAImgAE2I0ABMAqFEEDUAQgAJZ2gVEDBNiMAATAKowCA6AIQACskpTwQw3QKZ8jPh+tQICtCEAArJKU+OPb3kkfI8EAWxGAAFhZA6SoAwLsRQACYGUNkOKK8IC9CEAArJIQH2duirmAAHsRgABYWwjNFeEBexGAAFiH64EBSOQlAGBrIfS33x8LtAbBPSqnJUtqMh9fKBn+ggBY2wL062mror0pKIYKKYmy+IFr5ScVUnj9UGwEIADW6dqiury84htxmAfRdXJP+eRI7in5fM8RAhBKhAAEwDojOzc2N7hPt8kfyYYdhyQnLz/amwKXowgaAOAaaSkJ5uuxvFPR3hS4HAEIAOAa/uLnnFxagFAyBCAAgGukJdMChMggAAEAXCM15YcWoKO5dIGhZAhAAAAXtgDRBYaSIQABAFxYA0QLEEqGAAQAcI0LCrrAaAFCSRGAAACukVowDJ4WIJQUAQgA4BppBV1gtAChpAhAAADXSC0ogs5hIkSUEAEIAOAaaf4aICZCRAkRgAAArkELECKFAAQAcF8LEPMAoYQIQAAA97UAMQ8QvBCAJk+eLHXr1pVy5cpJVlaWrFq16pzrz5o1Sxo1amTWb9asmcydOzfw2MmTJ+XBBx80y9PS0qR69erSq1cv2bVrVxnsCQCgNKUVjALLPeWTU/k+Xmy4NwDNnDlThg4dKmPHjpV169ZJ8+bNpWPHjrJ3794i11++fLn06NFD+vbtK+vXr5du3bqZ28aNG83jx44dM88zevRo8/Wtt96SrVu3SteuXct4zwAApTUPkMqhGwwlEOc4jiNRpC0+bdq0kUmTJpn7Pp9PatWqJYMGDZKHHnrojPW7d+8uOTk58u677waW/fSnP5UWLVrIlClTivwdq1evlrZt28q2bdukdu3aZzyem5trbn6HDx8225CdnS3p6ekR2lMAQEnpR1bDh9+XUz5HVoz4uVyUUZ4XFUGf3xkZGSF9fke1BSgvL0/Wrl0rHTp0+HGD4uPN/RUrVhT5M7q88PpKW4zOtr7SFyIuLk4qVqxY5OPjxo0zL5j/puEHABB79L38xzogLoiK4otqANq/f7/k5+dL1apVg5br/d27dxf5M7o8nPVPnDhhaoK02+xsaXDEiBEmJPlvO3bsKPY+AQDK6npgXBAVxffDX5FHaUH0HXfcYZpMn3/++bOul5KSYm4AgNiXWhCAaAGCawNQZmamJCQkyJ49e4KW6/1q1aoV+TO6PJT1/eFH634WL15MLQ8AeERaQRcYLUBwbRdYcnKytGrVShYtWhRYpkXQer9du3ZF/owuL7y+WrBgQdD6/vDz+eefy8KFC6VKlSqluBcAgLKUWjAUnlFgcHUXmA6B7927t7Ru3dqM1Jo4caIZ5dWnTx/zuM7hU6NGDVOorIYMGSLt27eX8ePHS5cuXWTGjBmyZs0amTp1aiD8/OpXvzJD4HWkmNYY+euDKleubEIXAMC90gqGwh9jMkS4OQDpsPZ9+/bJmDFjTFDR4ezz5s0LFDpv377djAzzu/LKK+W1116TUaNGyciRI6Vhw4YyZ84cadq0qXl8586d8vbbb5vv9bkKW7JkiVx77bVlun8AgMiiBQiemAfI7fMIAADK1oi3PpbXV+2QYb+4RAZd35CXH+6bBwgAgHDRAoRIIAABAFyFUWCIBAIQAMCV8wAdpQgaJUAAAgC4swWIS2GgBAhAAACX1gBxKQwUHwEIAOAqaYFrgXExVBQfAQgA4MqJEHOoAUIJEIAAAK7sAqMFCCVBAAIAuPNSGNQAoQQIQAAAV0nzF0EzCgwlQAACALhKasEw+OMn8yXfx9WcUDwEIACAK0eB+UMQUBwEIACAq6Qkxkt83A/fH2MkGIqJAAQAcJW4uLgf64CYCwjFRAACALhOKnMBoYQIQAAA10kLjATjchgoHgIQAMC1LUBMhojiIgABAFyHC6KipAhAAADXucB/QVQmQ0Qx/TiZAgAALpsMceeh4/Lt98eivTkohgopSZKRmiTRQgACALhOWkER9LOLPjc3uM/d114sv+/UKGq/nwAEAHCdXzSpKvM37ZbjzAPkWon+2SyjJM5xHC6kcprDhw9LRkaGZGdnS3p6enSODAAAKLXPb4qgAQCAdQhAAADAOgQgAABgHQIQAACwDgEIAABYhwAEAACsQwACAADWIQABAADrEIAAAIB1CEAAAMA6BCAAAGAdAhAAALAOAQgAAFiHAAQAAKyTGO0NiEWO45ivhw8fjvamAACAEPk/t/2f4+dCACrCkSNHzNdatWqF+poDAIAY+hzPyMg45zpxTigxyTI+n0927dolFSpUkLi4uIinUw1WO3bskPT0dPEar++fYh/dj2PofhxDbzgc4c8MjTQafqpXry7x8eeu8qEFqAj6otWsWVNKkx5orwYEG/ZPsY/uxzF0P46hN6RH8DPjfC0/fhRBAwAA6xCAAACAdQhAZSwlJUXGjh1rvnqR1/dPsY/uxzF0P46hN6RE8TODImgAAGAdWoAAAIB1CEAAAMA6BCAAAGAdAhAAALAOAagMTZ48WerWrSvlypWTrKwsWbVqlbjRuHHjpE2bNmam7AsvvFC6desmW7duDVrn2muvNbNoF74NGDBA3OIPf/jDGdvfqFGjwOMnTpyQe+65R6pUqSIXXHCB3HbbbbJnzx5xE/1bPH0f9ab75dZj+O9//1tuuukmMwusbu+cOXPOmCV2zJgxctFFF0n58uWlQ4cO8vnnnwetc/DgQenZs6eZlK1ixYrSt29fOXr0qMT6/p08eVIefPBBadasmaSlpZl1evXqZWa1P99xf+KJJ8Qtx/Cuu+46Y/s7derkiWOoijon9fbUU0+55hiOC+EzIpT30O3bt0uXLl0kNTXVPM/w4cPl1KlTEdtOAlAZmTlzpgwdOtQM91u3bp00b95cOnbsKHv37hW3WbZsmfnDXblypSxYsMC88d5www2Sk5MTtF6/fv3ku+++C9yefPJJcZPLLrssaPs//PDDwGP333+/vPPOOzJr1izzeuiHzK233ipusnr16qD902Opbr/9dtceQ/0b1HNL/9koim7/c889J1OmTJH//Oc/Jijoeahvxn76wfnpp5+a1+Pdd981H1j9+/eXWN+/Y8eOmfeW0aNHm69vvfWW+dDp2rXrGes++uijQcd10KBB4pZjqDTwFN7+119/Pehxtx5DVXi/9Pbiiy+agKMBwS3HcFkInxHnew/Nz8834ScvL0+WL18uf/vb32T69OnmH5iI0WuBofS1bdvWueeeewL38/PznerVqzvjxo1z/cu/d+9evZ6cs2zZssCy9u3bO0OGDHHcauzYsU7z5s2LfOzQoUNOUlKSM2vWrMCyzZs3m9dgxYoVjlvp8br44osdn8/niWOox2P27NmB+7pf1apVc5566qmgY5mSkuK8/vrr5v6mTZvMz61evTqwzvvvv+/ExcU5O3fudGJ5/4qyatUqs962bdsCy+rUqeM888wzjhsUtY+9e/d2br755rP+jNeOoe7rz3/+86BlbjqGRX1GhPIeOnfuXCc+Pt7ZvXt3YJ3nn3/eSU9Pd3Jzc51IoAWoDGiCXbt2rWluL3y9Mb2/YsUKcbvs7GzztXLlykHLX331VcnMzJSmTZvKiBEjzH+obqJdI9pMXb9+ffMfpTbHKj2W+h9N4eOp3WO1a9d27fHUv9FXXnlFfvOb3wRdANjtx7Cwr7/+Wnbv3h103PSaQdod7T9u+lW7TFq3bh1YR9fX81VbjNx4burx1H0qTLtLtOuhZcuWpmslkt0KZWHp0qWmS+TSSy+VgQMHyoEDBwKPeekYapfQe++9Z7rwTuemY5h92mdEKO+h+lW7c6tWrRpYR1tr9eKp2roXCVwMtQzs37/fNOcVPpBK72/ZskXczOfzyX333Sc/+9nPzIek3//8z/9InTp1TID4+OOPTW2CNsdrs7wb6IeiNrfqG6w2Lz/yyCNy9dVXy8aNG82HaHJy8hkfKno89TE30jqEQ4cOmfoKrxzD0/mPTVHnof8x/aofrIUlJiaaN263HVvt1tNj1qNHj6CLTA4ePFiuuOIKs0/ataDBVv/GJ0yYIG6g3V/aVVKvXj358ssvZeTIkXLjjTeaD8yEhARPHUPt9tE6mtO71910DH1FfEaE8h6qX4s6V/2PRQIBCCWi/bwaCgrXx6jC/e2a4rXo9PrrrzdvWBdffHHMv+r6hup3+eWXm0CkYeCNN94wxbNeM23aNLPPGna8cgxtpv9d33HHHabo+/nnnw96TGsRC/9t6wfR7373O1O46oZL2Nx5551Bf5e6D/r3qK1C+vfpJVr/o63POnDGrcfwnrN8RsQCusDKgHYh6H8mp1e46/1q1aqJW917772mwHDJkiVSs2bNc66rAUJ98cUX4kb6n8oll1xitl+PmXYZaYuJF47ntm3bZOHChfLb3/7W08fQf2zOdR7q19MHJmjXgo4qcsux9YcfPa5agFq49edsx1X38ZtvvhE30i5qfY/1/1164RiqDz74wLS4nu+8jOVjeO9ZPiNCeQ/Vr0Wdq/7HIoEAVAY0nbdq1UoWLVoU1Cyo99u1ayduo/9V6h/27NmzZfHixaYp+nw2bNhgvmorghvpEFpt+dDt12OZlJQUdDz1jUprhNx4PF966SXTZaAjLrx8DPXvVN84Cx83rSfQuhD/cdOv+qasNQp++jeu56s/ALoh/Gj9moZarRE5Hz2uWh9zereRW3z77bemBsj/d+n2Y1i4VVbfa3TEmNuOoXOez4hQ3kP16yeffBIUZv2BvkmTJhHbUJSBGTNmmNEm06dPN6MU+vfv71SsWDGowt0tBg4c6GRkZDhLly51vvvuu8Dt2LFj5vEvvvjCefTRR501a9Y4X3/9tfPPf/7TqV+/vnPNNdc4bjFs2DCzf7r9H330kdOhQwcnMzPTjGZQAwYMcGrXru0sXrzY7Ge7du3MzW10NKLux4MPPhi03K3H8MiRI8769evNTd/eJkyYYL73j4J64oknzHmn+/Pxxx+bETb16tVzjh8/HniOTp06OS1btnT+85//OB9++KHTsGFDp0ePHk6s719eXp7TtWtXp2bNms6GDRuCzk3/qJnly5eb0UP6+Jdffum88sorzk9+8hOnV69eTqw41z7qYw888IAZKaR/lwsXLnSuuOIKc4xOnDjh+mPol52d7aSmpppRT6dzwzEceJ7PiFDeQ0+dOuU0bdrUueGGG8y+zps3z+zniBEjIradBKAy9Je//MUc8OTkZDMsfuXKlY4b6Ulb1O2ll14yj2/fvt18UFauXNmEvgYNGjjDhw83J7VbdO/e3bnooovMsapRo4a5r6HATz8w7777bqdSpUrmjeqWW24xJ7jbzJ8/3xy7rVu3Bi136zFcsmRJkX+bOnTaPxR+9OjRTtWqVc1+XX/99Wfs+4EDB8yH5QUXXGCG3Pbp08d8aMX6/mkgONu5qT+n1q5d62RlZZkPp3LlyjmNGzd2/vSnPwWFh1jeR/0A1Q9E/SDUYdQ6HLxfv35n/CPp1mPo98ILLzjly5c3w8VP54ZjKOf5jAj1PfSbb75xbrzxRvNa6D+g+o/pyZMnI7adcQUbCwAAYA1qgAAAgHUIQAAAwDoEIAAAYB0CEAAAsA4BCAAAWIcABAAArEMAAgAA1iEAAQAA6xCAACAEcXFxMmfOHF4rwCMIQABi3l133WUCyOm3Tp06RXvTALhUYrQ3AABCoWFHr1xfWEpKCi8egGKhBQiAK2jYqVatWtCtUqVK5jFtDXr++eflxhtvlPLly0v9+vXlzTffDPr5Tz75RH7+85+bx6tUqSL9+/eXo0ePBq3z4osvymWXXWZ+10UXXST33ntv0OP79++XW265RVJTU6Vhw4by9ttvl8GeAygNBCAAnjB69Gi57bbb5L///a/07NlT7rzzTtm8ebN5LCcnRzp27GgC0+rVq2XWrFmycOHCoICjAeqee+4xwUjDkoabBg0aBP2ORx55RO644w75+OOPpXPnzub3HDx4sMz3FUAEROy68gBQSnr37u0kJCQ4aWlpQbfHH3/cPK5vZQMGDAj6maysLGfgwIHm+6lTpzqVKlVyjh49Gnj8vffec+Lj453du3eb+9WrV3cefvjhs26D/o5Ro0YF7utz6bL3338/4vsLoPRRAwTAFa677jrTSlNY5cqVA9+3a9cu6DG9v2HDBvO9tgQ1b95c0tLSAo//7Gc/E5/PJ1u3bjVdaLt27ZLrr7/+nNtw+eWXB77X50pPT5e9e/eWeN8AlD0CEABX0MBxepdUpGhdUCiSkpKC7mtw0hAFwH2oAQLgCStXrjzjfuPGjc33+lVrg7QWyO+jjz6S+Ph4ufTSS6VChQpSt25dWbRoUZlvN4DooAUIgCvk5ubK7t27g5YlJiZKZmam+V4Lm1u3bi1XXXWVvPrqq7Jq1SqZNm2aeUyLlceOHSu9e/eWP/zhD7Jv3z4ZNGiQ/PrXv5aqVauadXT5gAED5MILLzSjyY4cOWJCkq4HwHsIQABcYd68eWZoemHaerNly5bACK0ZM2bI3XffbdZ7/fXXpUmTJuYxHbY+f/58GTJkiLRp08bc1xFjEyZMCDyXhqMTJ07IM888Iw888IAJVr/61a/KeC8BlJU4rYQus98GAKVAa3Fmz54t3bp14/UFEBJqgAAAgHUIQAAAwDrUAAFwPXryAYSLFiAAAGAdAhAAALAOAQgAAFiHAAQAAKxDAAIAANYhAAEAAOsQgAAAgHUIQAAAQGzz/41IyGHYtDR8AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_list: [[0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.1], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.05], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.025], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125], [0.0125]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_list: [[0.1], [0.095], [0.09025], [0.0857375], [0.08145062499999998], [0.07737809374999999], [0.07350918906249998], [0.06983372960937498], [0.06634204312890622], [0.0630249409724609], [0.05987369392383786], [0.05688000922764597], [0.05403600876626367], [0.05133420832795048], [0.04876749791155295], [0.046329123015975304], [0.04401266686517654], [0.04181203352191771], [0.039721431845821824], [0.037735360253530734], [0.035848592240854196], [0.03405616262881148], [0.03235335449737091], [0.030735686772502362], [0.029198902433877242], [0.027738957312183378], [0.026352009446574207], [0.025034408974245494], [0.023782688525533217], [0.022593554099256556], [0.021463876394293726], [0.020390682574579037], [0.019371148445850084], [0.01840259102355758], [0.0174824614723797], [0.016608338398760712], [0.015777921478822676], [0.014989025404881541], [0.014239574134637464], [0.01352759542790559], [0.012851215656510309], [0.012208654873684792], [0.011598222130000552], [0.011018311023500524], [0.010467395472325497], [0.009944025698709221], [0.00944682441377376], [0.00897448319308507], [0.008525759033430816], [0.008099471081759275], [0.007694497527671311], [0.007309772651287745], [0.006944284018723357], [0.006597069817787189], [0.006267216326897829], [0.005953855510552938], [0.005656162735025291], [0.005373354598274026], [0.005104686868360324], [0.004849452524942308], [0.004606979898695193], [0.004376630903760433], [0.0041577993585724116], [0.0039499093906437905], [0.0037524139211116006], [0.0035647932250560204], [0.003386553563803219], [0.003217225885613058], [0.0030563645913324047], [0.0029035463617657843], [0.002758369043677495], [0.00262045059149362], [0.002489428061918939], [0.002364956658822992], [0.002246708825881842], [0.00213437338458775], [0.0020276547153583622], [0.001926271979590444], [0.0018299583806109217], [0.0017384604615803755], [0.0016515374385013568], [0.0015689605665762888], [0.0014905125382474742], [0.0014159869113351004], [0.0013451875657683454], [0.001277928187479928], [0.0012140317781059316], [0.001153330189200635], [0.0010956636797406032], [0.001040880495753573], [0.0009888364709658944], [0.0009393946474175996], [0.0008924249150467197], [0.0008478036692943836], [0.0008054134858296644], [0.0007651428115381812], [0.000726885670961272], [0.0006905413874132084], [0.0006560143180425479], [0.0006232136021404205], [0.0005920529220333994], [0.0005624502759317294], [0.0005343277621351429], [0.0005076113740283857], [0.00048223080532696635], [0.00045811926506061804], [0.0004352133018075871], [0.00041345263671720774], [0.00039278000488134735], [0.00037314100463728], [0.000354483954405416], [0.00033675975668514516], [0.0003199217688508879], [0.00030392568040834347], [0.0002887293963879263], [0.00027429292656852995], [0.00026057828024010345], [0.00024754936622809826], [0.00023517189791669334], [0.00022341330302085867], [0.00021224263786981574], [0.00020163050597632494], [0.0001915489806775087], [0.00018197153164363326], [0.0001728729550614516], [0.00016422930730837902], [0.00015601784194296006], [0.00014821694984581206], [0.00014080610235352146], [0.0001337657972358454], [0.0001270775073740531], [0.00012072363200535044], [0.00011468745040508291], [0.00010895307788482875], [0.00010350542399058731], [9.833015279105794e-05], [9.341364515150504e-05], [8.874296289392978e-05], [8.430581474923329e-05], [8.009052401177162e-05], [7.608599781118304e-05], [7.228169792062389e-05], [6.866761302459269e-05], [6.523423237336306e-05], [6.19725207546949e-05], [5.8873894716960144e-05], [5.5930199981112136e-05], [5.3133689982056524e-05], [5.0477005482953695e-05], [4.7953155208806006e-05], [4.55554974483657e-05], [4.327772257594741e-05], [4.111383644715004e-05], [3.9058144624792534e-05], [3.7105237393552906e-05], [3.524997552387526e-05], [3.34874767476815e-05], [3.1813102910297426e-05], [3.0222447764782554e-05], [2.8711325376543424e-05], [2.727575910771625e-05], [2.5911971152330435e-05], [2.461637259471391e-05], [2.3385553964978216e-05], [2.2216276266729303e-05], [2.1105462453392836e-05], [2.0050189330723194e-05], [1.9047679864187035e-05], [1.8095295870977683e-05], [1.71905310774288e-05], [1.6331004523557357e-05], [1.5514454297379488e-05], [1.4738731582510512e-05], [1.4001795003384986e-05], [1.3301705253215736e-05], [1.2636619990554949e-05], [1.2004788991027201e-05], [1.140454954147584e-05], [1.0834322064402047e-05], [1.0292605961181944e-05], [9.777975663122847e-06], [9.289076879966705e-06], [8.82462303596837e-06], [8.38339188416995e-06], [7.964222289961452e-06], [7.566011175463379e-06], [7.18771061669021e-06], [6.8283250858556995e-06], [6.4869088315629144e-06], [6.162563389984768e-06], [5.85443522048553e-06], [5.561713459461253e-06], [5.28362778648819e-06], [5.019446397163781e-06], [4.768474077305592e-06], [4.530050373440312e-06], [4.303547854768296e-06], [4.088370462029881e-06], [3.883951938928387e-06], [3.689754341981967e-06]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "案例:\n",
    "    演示学习率衰减策略.\n",
    "\n",
    "学习率衰减策略介绍:\n",
    "    目的:\n",
    "        较之于AdaGrad, RMSProp, Adam方式, 我们可以通过 等间隔, 指定间隔, 指数等方式, 来手动控制学习率的调整.\n",
    "\n",
    "    分类:\n",
    "        等间隔学习率衰减\n",
    "        指定间隔学习率衰减\n",
    "        指数学习率衰减\n",
    "\n",
    "等间隔学习率衰减:\n",
    "    step_size: 间隔的轮数, 即: 多少轮调整一次学习率.\n",
    "    gamma:     学习率衰减系数, 即: lr新 = lr旧 * gamma\n",
    "\n",
    "指定间隔学习率衰减:\n",
    "    milestones = [50, 125, 160]     里边定义的是要调整学习率的 轮数.\n",
    "    gamma:     学习率衰减系数, 即: lr新 = lr旧 * gamma\n",
    "\n",
    "指数间隔学习率衰减:\n",
    "    前期学习率衰减快, 中期慢, 后期更慢, 更符合梯度下降规律.\n",
    "    公式:\n",
    "        lr新 = lr旧 * gamma ** epoch\n",
    "\n",
    "总结:\n",
    "    等间隔学习率衰减:\n",
    "        优点:\n",
    "            直观, 易于调试, 适用于 大批量数据.\n",
    "        缺点:\n",
    "            学习率变化较大, 可能跳过最优解.\n",
    "        应用场景:\n",
    "            大型数据集, 较为简单的任务.\n",
    "\n",
    "    指定间学习率衰减:\n",
    "        优点:\n",
    "            易于调试, 稳定训练过程.\n",
    "        缺点:\n",
    "            在某些情况下可能衰减过快, 导致优化提前停滞.\n",
    "        应用场景:\n",
    "            对训练平稳性要求较高的任务.\n",
    "   指数学习率衰减:\n",
    "        优点:\n",
    "            平滑, 且考虑历史更新, 收敛稳定性较强.\n",
    "        缺点:\n",
    "            超参调节较为复杂, 可能需要更多的资源.\n",
    "        应用场景:\n",
    "            高精度训练, 避免过快收敛.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# 导包\n",
    "import torch\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 1. 定义函数, 演示: 等间隔学习率衰减\n",
    "def dm01():\n",
    "    # 1. 定义变量, 记录初始的 学习率, 训练的轮数, 每轮训练的批次数.\n",
    "    lr, epochs, iteration = 0.1, 200, 10\n",
    "\n",
    "    # 2. 创建数据集.  y_true, x, w\n",
    "    # 真实值.\n",
    "    y_true = torch.tensor([0])\n",
    "    # 输入特征\n",
    "    x = torch.tensor([1.0], dtype=torch.float32)\n",
    "    # 权重参数w, 需要自动微分(求导)\n",
    "    w = torch.tensor([1.0], requires_grad=True, dtype=torch.float32)\n",
    "\n",
    "    # 3. 创建优化器对象, 动量法 -> 加速模型的收敛, 减少震荡.\n",
    "    # 参1: 待优化的参数, 参2: 学习率, 参3: 动量系数\n",
    "    optimizer = optim.SGD([w], lr=lr, momentum=0.9)\n",
    "\n",
    "    # 4. 创建学习率衰减对象.\n",
    "    # 思路1: 创建等间隔学习率衰减对象.\n",
    "    # 参1: 优化器对象, 参2: 间隔的轮数(多少轮调整一次学习率), 参3: 学习率衰减系数.\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)   # [0.1, 0.1, 0.1... 0.05...]\n",
    "\n",
    "    # 5. 创建两个列表, 分别表示: 训练轮数, 每轮训练用的学习率\n",
    "    # epoch_list = [0, 1, 2, 3.... 50, 51, 52...100, 101, 101... 150, 151...199]\n",
    "    # lr_list =    [0.1, 0.1, 0.1, 0.05........,0.025.........,  0.0125...]\n",
    "    lr_list, epoch_list = [], []\n",
    "\n",
    "    # 6. 循环遍历训练轮数, 进行具体的训练.\n",
    "    for epoch in range(epochs):     # epoch: 0 ~ 199\n",
    "        # 7. 获取当前轮数 和 学习率, 并保存到列表中.\n",
    "        epoch_list.append(epoch)\n",
    "        lr_list.append(scheduler.get_last_lr())     # 获取最后的lr(learning rate, 学习率)\n",
    "\n",
    "        # 8. 循环遍历, 每轮每批次进行训练.\n",
    "        for batch in range(iteration):\n",
    "            # 9. 先计算预测值, 然后基于损失函数计算损失.\n",
    "            y_pred = w * x\n",
    "            # 10. 计算损失, 最小二乘法.\n",
    "            loss = (y_pred - y_true) ** 2\n",
    "            # 11. 梯度清零 + 反向传播 + 优化器更新参数.\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # 12. 更新学习率.\n",
    "        scheduler.step()\n",
    "    # 13. 打印结果:\n",
    "    print(f'lr_list: {lr_list}')        # [0.1, 0.1, 0.1..., 0.05........,0.025.........,  0.0125...]\n",
    "\n",
    "    # 14. 可视化.\n",
    "    # x轴: 训练的轮数, y轴: 每轮训练用的学习率\n",
    "    plt.plot(epoch_list, lr_list)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 2. 定义函数, 演示: 指定间隔学习率衰减\n",
    "def dm02():\n",
    "    # 1. 定义变量, 记录初始的 学习率, 训练的轮数, 每轮训练的批次数.\n",
    "    lr, epochs, iteration = 0.1, 200, 10\n",
    "\n",
    "    # 2. 创建数据集.  y_true, x, w\n",
    "    # 真实值.\n",
    "    y_true = torch.tensor([0])\n",
    "    # 输入特征\n",
    "    x = torch.tensor([1.0], dtype=torch.float32)\n",
    "    # 权重参数w, 需要自动微分(求导)\n",
    "    w = torch.tensor([1.0], requires_grad=True, dtype=torch.float32)\n",
    "\n",
    "    # 3. 创建优化器对象, 动量法 -> 加速模型的收敛, 减少震荡.\n",
    "    # 参1: 待优化的参数, 参2: 学习率, 参3: 动量系数\n",
    "    optimizer = optim.SGD([w], lr=lr, momentum=0.9)\n",
    "\n",
    "    # 4. 创建学习率衰减对象.\n",
    "    # 思路1: 创建等间隔学习率衰减对象.\n",
    "    # 参1: 优化器对象, 参2: 间隔的轮数(多少轮调整一次学习率), 参3: 学习率衰减系数.\n",
    "    # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)   # [0.1, 0.1, 0.1... 0.05...]\n",
    "\n",
    "    # 思路2: 创建指定间隔学习率衰减对象.\n",
    "    # 定义变量, 记录要修改学习率的轮数.\n",
    "    milestones = [50, 125, 160]\n",
    "    scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.5)\n",
    "\n",
    "    # 5. 创建两个列表, 分别表示: 训练轮数, 每轮训练用的学习率\n",
    "    # epoch_list = [0, 1, 2, 3.... 50, 51, 52...100, 101, 101... 150, 151...199]\n",
    "    # lr_list =    [0.1, 0.1, 0.1, 0.05........,0.025.........,  0.0125...]\n",
    "    lr_list, epoch_list = [], []\n",
    "\n",
    "    # 6. 循环遍历训练轮数, 进行具体的训练.\n",
    "    for epoch in range(epochs):     # epoch: 0 ~ 199\n",
    "        # 7. 获取当前轮数 和 学习率, 并保存到列表中.\n",
    "        epoch_list.append(epoch)\n",
    "        lr_list.append(scheduler.get_last_lr())     # 获取最后的lr(learning rate, 学习率)\n",
    "\n",
    "        # 8. 循环遍历, 每轮每批次进行训练.\n",
    "        for batch in range(iteration):\n",
    "            # 9. 先计算预测值, 然后基于损失函数计算损失.\n",
    "            y_pred = w * x\n",
    "            # 10. 计算损失, 最小二乘法.\n",
    "            loss = (y_pred - y_true) ** 2\n",
    "            # 11. 梯度清零 + 反向传播 + 优化器更新参数.\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # 12. 更新学习率.\n",
    "        scheduler.step()\n",
    "    # 13. 打印结果:\n",
    "    print(f'lr_list: {lr_list}')        # [0.1, 0.1, 0.1..., 0.05........,0.025.........,  0.0125...]\n",
    "\n",
    "    # 14. 可视化.\n",
    "    # x轴: 训练的轮数, y轴: 每轮训练用的学习率\n",
    "    plt.plot(epoch_list, lr_list)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 3. 定义函数, 演示: 指数学习率衰减\n",
    "def dm03():\n",
    "    # 1. 定义变量, 记录初始的 学习率, 训练的轮数, 每轮训练的批次数.\n",
    "    lr, epochs, iteration = 0.1, 200, 10\n",
    "\n",
    "    # 2. 创建数据集.  y_true, x, w\n",
    "    # 真实值.\n",
    "    y_true = torch.tensor([0])\n",
    "    # 输入特征\n",
    "    x = torch.tensor([1.0], dtype=torch.float32)\n",
    "    # 权重参数w, 需要自动微分(求导)\n",
    "    w = torch.tensor([1.0], requires_grad=True, dtype=torch.float32)\n",
    "\n",
    "    # 3. 创建优化器对象, 动量法 -> 加速模型的收敛, 减少震荡.\n",
    "    # 参1: 待优化的参数, 参2: 学习率, 参3: 动量系数\n",
    "    optimizer = optim.SGD([w], lr=lr, momentum=0.9)\n",
    "\n",
    "    # 4. 创建学习率衰减对象.\n",
    "    # 思路1: 创建等间隔学习率衰减对象.\n",
    "    # 参1: 优化器对象, 参2: 间隔的轮数(多少轮调整一次学习率), 参3: 学习率衰减系数.\n",
    "    # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)   # [0.1, 0.1, 0.1... 0.05...]\n",
    "\n",
    "    # 思路2: 创建指定间隔学习率衰减对象.\n",
    "    # 定义变量, 记录要修改学习率的轮数.\n",
    "    # milestones = [50, 125, 160]\n",
    "    # scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=milestones, gamma=0.5)\n",
    "\n",
    "    # 思路3: 创建指数学习率衰减对象.\n",
    "    scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "\n",
    "    # 5. 创建两个列表, 分别表示: 训练轮数, 每轮训练用的学习率\n",
    "    # epoch_list = [0, 1, 2, 3.... 50, 51, 52...100, 101, 101... 150, 151...199]\n",
    "    # lr_list =    [0.1, 0.1, 0.1, 0.05........,0.025.........,  0.0125...]\n",
    "    lr_list, epoch_list = [], []\n",
    "\n",
    "    # 6. 循环遍历训练轮数, 进行具体的训练.\n",
    "    for epoch in range(epochs):     # epoch: 0 ~ 199\n",
    "        # 7. 获取当前轮数 和 学习率, 并保存到列表中.\n",
    "        epoch_list.append(epoch)\n",
    "        lr_list.append(scheduler.get_last_lr())     # 获取最后的lr(learning rate, 学习率)\n",
    "\n",
    "        # 8. 循环遍历, 每轮每批次进行训练.\n",
    "        for batch in range(iteration):\n",
    "            # 9. 先计算预测值, 然后基于损失函数计算损失.\n",
    "            y_pred = w * x\n",
    "            # 10. 计算损失, 最小二乘法.\n",
    "            loss = (y_pred - y_true) ** 2\n",
    "            # 11. 梯度清零 + 反向传播 + 优化器更新参数.\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # 12. 更新学习率.\n",
    "        scheduler.step()\n",
    "    # 13. 打印结果:\n",
    "    print(f'lr_list: {lr_list}')        # [0.1, 0.1, 0.1..., 0.05........,0.025.........,  0.0125...]\n",
    "\n",
    "    # 14. 可视化.\n",
    "    # x轴: 训练的轮数, y轴: 每轮训练用的学习率\n",
    "    plt.plot(epoch_list, lr_list)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 4. 测试\n",
    "if __name__ == '__main__':\n",
    "    dm01()\n",
    "    dm02()\n",
    "    dm03()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845c4687c493466e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "案例:\n",
    "    代码演示 随机失活.\n",
    "\n",
    "正则化的作用:\n",
    "    缓解模型的过拟合情况.\n",
    "\n",
    "正则化的方式:\n",
    "    L1正则化: 权重可以变为0, 相当于: 降维.\n",
    "    L2正则化: 权重可以无限接近0\n",
    "    DropOut: 随机失活, 每批次样本训练时, 随机让一部分神经元死亡, 防止一些特征对结果的影响较大(防止过拟合)\n",
    "    BN(批量归一化): ...\n",
    "\"\"\"\n",
    "\n",
    "# 导包\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# 1. 定义函数, 演示: 随机失活(DropOut)\n",
    "def dm01():\n",
    "    # 1. 创建隐藏层输出结果.\n",
    "    t1 = torch.randint(0, 10, size=(1, 4)).float()\n",
    "    print(f't1: {t1}')      # t1: tensor([[0., 5., 6., 3.]])\n",
    "\n",
    "    # 2. 进行下一层 加权求和 和 激活函数计算.\n",
    "    # 2.1 创建全连接层(充当线性层)\n",
    "    # 参1: 输入特征维度, 参2: 输出特征维度.\n",
    "    linear1 = nn.Linear(4, 5)\n",
    "\n",
    "    # 2.2 加权求和.\n",
    "    l1 = linear1(t1)\n",
    "    print(f'l1: {l1}')\n",
    "\n",
    "    # 2.3 激活函数.\n",
    "    output = torch.relu(l1)\n",
    "    print(f'output: {output}')\n",
    "\n",
    "    # 3. 对激活值进行随机失活dropout处理 -> 只有训练阶段有, 测试阶段没有.\n",
    "    dropout = nn.Dropout(p=0.5) # 每个神经元都有50%的概率被 kill.\n",
    "    # 具体的 随机失活动作.\n",
    "    d1 = dropout(output)\n",
    "    print(f'd1(随机失活后的数据): {d1}')        # 未被失活的进行缩放, 缩放比例为: 1 / (1 - p) = 2\n",
    "\n",
    "\n",
    "# 2. 测试\n",
    "if __name__ == '__main__':\n",
    "    dm01()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf9e8acc885a789",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T11:15:42.471070Z",
     "start_time": "2026-01-30T11:15:42.458151Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "案例:\n",
    "    代码演示批量归一化,  它(批量归一化)也属于正则化的一种, 也是用于 缓解模型的 过拟合情况的.\n",
    "\n",
    "批量归一化:\n",
    "    思路:\n",
    "        先对数据做标准化(会丢失一些信息), 然后再对数据做 缩放(λ, 理解为: w权重) 和 平移(β, 理解为: b偏置), 再找补回一些信息.\n",
    "    应用场景:\n",
    "        批量归一化在计算机视觉领域使用较多.\n",
    "\n",
    "        BatchNorm1d：主要应用于全连接层或处理一维数据的网络，例如文本处理。它接收形状为 (N, num_features) 的张量作为输入。\n",
    "        BatchNorm2d：主要应用于卷积神经网络，处理二维图像数据或特征图。它接收形状为 (N, C, H, W) 的张量作为输入。\n",
    "        BatchNorm3d：主要用于三维卷积神经网络 (3D CNN)，处理三维数据，例如视频或医学图像。它接收形状为 (N, C, D, H, W) 的张量作为输入。\n",
    "\"\"\"\n",
    "\n",
    "# 导包\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# 1. 定义函数, 处理 二维数据.\n",
    "def dm01():\n",
    "    # 1. 创建图像样本数据.\n",
    "    # 1张图片, 2个通道, 3行4列(像素点)\n",
    "    input_2d = torch.randn(size=(1, 2, 3, 4))\n",
    "    print(f'input_2d: {input_2d}')\n",
    "\n",
    "    # 2. 创建批量归一化层(BN层)\n",
    "    # 参1: 输入特征数 = 图片的通道数.\n",
    "    # 参2: 噪声值(小常数), 默认为1e-5.\n",
    "    # 参3: 动量值, 用于计算移动平局统计量的  动量值.\n",
    "    # 参4: 表示使用可学习的变换参数(λ, β) 对归一化(标准化)后的数据进行 缩放和平移.\n",
    "    bn2d = nn.BatchNorm2d(num_features=2, eps=1e-5, momentum=0.1, affine=True)\n",
    "\n",
    "    # 3. 对数据进行 批量归一化处理.\n",
    "    output_2d = bn2d(input_2d)\n",
    "    print(f'output_2d: {output_2d}')\n",
    "\n",
    "\n",
    "# 2. 定义函数, 处理: 一维数据.\n",
    "def dm02():\n",
    "    # 1. 创建样本数据.\n",
    "    # 2行2列, 2条样本, 每个样本有2个特征\n",
    "    input_1d = torch.randn(size=(2, 2))\n",
    "    print(f'input_1d: {input_1d}')\n",
    "\n",
    "    # 2. 创建线性层.\n",
    "    linear1 = nn.Linear(2, 4)\n",
    "\n",
    "    # 3. 对数据进行 线性变换.\n",
    "    l1 = linear1(input_1d)\n",
    "    print(f'l1: {l1}')\n",
    "\n",
    "    # 4. 创建批量归一化层.\n",
    "    bn1d = nn.BatchNorm1d(num_features=4)\n",
    "    # 5. 对线性处理结果l1 进行 批量归一化处理.\n",
    "    output_1d = bn1d(l1)\n",
    "    print(f'output_1d: {output_1d}')\n",
    "\n",
    "\n",
    "\n",
    "# 3. 测试\n",
    "if __name__ == '__main__':\n",
    "    dm01()\n",
    "    dm02()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0eae90f9689cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e4bc153dbf14c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
